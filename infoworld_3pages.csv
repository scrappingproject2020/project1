title,blurp,text,url
Julia vs. Python: Which is best for data science?,"Python has turned into a data science and machine learning mainstay, while Julia was built from the ground up to do the job","Among the many use cases Python covers, data analytics has become perhaps the biggest and most significant. The Python ecosystem is loaded with libraries, tools, and applications that make the work of scientific computing and data analysis fast and convenient.But for the developers behind Created in 2009 by a four-person team and unveiled to the public in 2012, Julia is meant to address the shortcomings in Python and other languages and applications used for scientific computing and data processing. “We are greedy,” they wrote. They wanted more: Here are some of the ways Julia implements those aspirations:Julia was designed from the start for scientific and numerical computation. Thus it’s no surprise that Julia has many features advantageous for such use cases:Although Julia is purpose-built for data science, whereas Python has more or less evolved into the role, Python offers some compelling advantages to the data scientist. Some of the reasons “general purpose” Python may be the better choice for data science work:",https://www.infoworld.com/article/3241107/julia-vs-python-which-is-best-for-data-science.html
Working with the Azure Kinect Developer Kit,Building applications on top of the Kinect depth sensor,"Microsoft announced Azure Kinect is a direct descendent of the second-generation Kinect modules that shipped with the Xbox One, but instead of providing real-world inputs for gaming, it’s targeted at enterprise users and applications. Intended to work with Azure’s Cognitive Services, the first Azure Kinect developer kit started shipping at the end of 2019 in the United States, adding several other countries in early 2020.The $399 Azure Kinect Developer Kit is a small white unit with two camera lenses, one for a wide-angle RGB camera and one for the Kinect depth sensor, and an array of microphones. It has an orientation sensor, allowing you to use the camera to build complex 3-D images of environments, ready for use in mixed reality. You can chain multiple devices together for quick 3-D scans or to provide coverage of an entire room, using the orientation sensor to help understand device position.Along with the camera unit, you get a power supply, an Allen key to remove the chaining ports cover, and a USB cable to connect to a development PC. I’d recommend getting a desktop tripod or another type of mount, as the bundled plastic stand is rather small and doesn’t work with most desks or monitors. There’s no software in the box, only Before you get started, you should update the device firmware. This ships with the SDK and includes a command line installation tool. When you run the updater it first checks the current firmware state before installing camera and device firmware and then rebooting. Once the camera has rebooted, use the same tool to check that the update has installed successfully. If there’s a problem with an install you can use the camera’s hardware reset (hidden under the tripod mount) to restore the original factory image.With the SDK installedIt’s a good idea to spend some time playing with Microsoft provides The Azure Kinect Developer Kit only delivers streaming data, so applications need to configure the data rate in frames per second, along with image color formats and resolutions. Once you’ve created a configuration object you can open a connection using your configuration object, ready to stream data. When you’re finished reading a data stream, stop and close the device.Images are captured in a capture objectA similar process gives you data from Although the Azure Kinect hardware captures a lot of data, the SDK functions help transform it into a usable form; for example, adding depth data to an RGB image to produce RGB-D images that are transformed to the viewpoint of the RGB camera (and vice versa). As the two sensors are off-set, this requires warping an image mesh to merge the two cameras’ viewpoints, using your PC’s GPU. Another transform generates a point cloud, allowing you to get depth data for each pixel in your capture. One useful option in the SDK is the ability to The original Kinect hardware introduced body tracking, with a skeletal model that could be used to quickly evaluate posture and gestures. That same approach continues in the The Body Tracking SDK builds on the Azure Kinect SDK, using it to configure and connect to a device. Captured image data is processed by the tracker, Azure’s Cognitive Services are a powerful tool for processing data, and the addition of Azure Kinect makes it possible to use them in a wide range of industrial and enterprise scenarios. With a focus on workplace 3-D image recognition, Microsoft is attempting to show how image recognition can be used to reduce risk and improve safety. There’s even the option of using ",https://www.infoworld.com/article/3562738/working-with-the-azure-kinect-developer-kit.html
3 ways to apply agile to data science and dataops,"Take an agile approach to dashboards, machine learning models, cleansing data sources, and data governance","Just about every organization is trying to become more data-driven, hoping to leverage data visualizations, analytics, and machine learning for competitive advantages. Providing actionable insights through analytics requires a strong Delivering dataops, analytics, and governance is a significant scope that requires aligning stakeholders on priorities, implementing multiple technologies, and gathering people with diverse backgrounds and skills. Agile methodologies can also help data and analytics teams capture and process feedback from customers, stakeholders, and end-users. Feedback should drive data visualization improvements, machine learning model recalibrations, data quality increases, and data governance compliance.  Applying agile methodologies to the analytics and machine learning lifecycle is a significant opportunity, but it requires redefining some terms and concepts. For example:An agile data science team is likely to have several types of work. Here are three primary ones that should fill backlogs and sprint commitments.Data science teams should conceive dashboards to help end-users answer questions. For example, a sales dashboard may answer the question, “What sales territories have seen the most sales activity by rep during the last 90 days?” A dashboard for agile software development teams may answer, “Over the last three releases, how productive has the team been delivering features, addressing technical debt, and resolving production defects?”Agile user stories should address three questionsIt then helps when stakeholders and end-users provide a hypothesis to an answer and how they intend to make the results actionable. How insights become actionable and their business impacts help answer the third question (why is the problem important) that agile user stories should address.The first version of a Tableau or Power BI dashboard should be a “minimal viable dashboard” that’s good enough to share with end-users to get feedback. Users should let the data science team know how well the dashboard addresses their questions and how to improve. The analytics product owner should put these enhancements on the backlog and consider prioritizing them in future sprints.The process of developing analytical and machine learning models includes segmenting and tagging data, feature extraction, and running data sets through multiple algorithms and configurations. Agile data science teams might record agile user stories for prepping data for use in model development and then creating separate stories for each experiment. The transparency helps teams review the results from experiments, decide on the next priorities, and discuss whether approaches are converging on beneficial results.There are likely separate user stories to move models from the lab into production environments. These stories are Once models are in production, the data science team has responsibilities to maintain them. As new data comes in, models may drift off target and require recalibration or re-engineering with updated data sets. Advanced machine learning teams from companies like Twitter and Facebook Agile data science teams should always seek out new data sources to integrate and enrich their strategic data warehouses and data lakes. One important example is Analyst owners should fill agile backlogs with story cards to research new data sources, validate sample data sets, and integrate prioritized ones into the primary data repositories. When agile teams integrate new data sources, the teams should consider automating the data integration, implementing data validation and quality rules, and linking data with master data sources.Julien Sauvage, vice president of product marketing at The data science team should also capture and prioritize Just like there isn’t a quick fix to address technical debt, agile data science groups should prioritize and address data debt iteratively. As the analytics owner adds user stories for delivering analytics, the team should review and ask what underlying data debt must be itemized on the backlog and prioritized.The examples I shared all help data science teams improve data quality and deliver tools for leveraging analytics in decision making, products, and services.In a proactive data governance program, issues around data policy, privacy, and security get prioritized and addressed in parallel to the work to deliver and improve data visualizations, analytics, machine learning, and dataops. Sometimes data governance work falls under the scope of data science teams, but more often, a separate group or function is responsible for data governance.Organizations have growing competitive needs around analytics and data governance regulations, compliance, and evolving best practices. Applying agile methodologies provides organizations with a well-established structure, process, and tools to prioritize, plan, and deliver data-driven impacts.",https://www.infoworld.com/article/3562346/3-ways-to-apply-agile-to-data-science-and-dataops.html
Quantum AI is still years from enterprise prime time,Quantum computing’s greatest potential for widespread adoption during this decade is in  artificial intelligence ,"Quantum computing’s potential to revolutionize AI depends on growth of a developer ecosystem in which suitable tools, skills, and platforms are in abundance. To be considered ready for enterprise production deployment, the quantum AI industry would have to, at the very least, reach the following key milestones:These milestones are all still at least a few years in the future. What follows is an analysis of the quantum AI industry’s maturity at the present time.Quantum AI executes ML (machine learning), DL (deep learning), and other data-driven AI algorithms reasonably well.As an approach, quantum AI has moved well beyond the proof-of-concept stage. However, that’s not the same as being able to claim that quantum approaches are superior to classical approaches for executing the matrix operations upon which AI’s inferencing and training workloads depend.Where AI is concerned, the key criterion is whether quantum platforms can accelerate ML and DL workloads faster than computers built entirely on classical von Neumann architectures. So far there is no specific AI application that a quantum computer can perform better than any classical alternative. For us to declare quantum AI a mature enterprise technology, there would need to be at least a few AI applications for which it offers a clear advantage—speed, accuracy, efficiency—over classical approaches to processing these workloads.Nevertheless, pioneers of quantum AI have aligned its functional processing algorithms with the mathematical properties of quantum computing architectures. Currently, the chief algorithmic approaches for quantum AI include:Leveraging these techniques, some current AI implementations use quantum platforms as coprocessors on select calculation workloads, such as autoencoders, GANs (generative adversarial networks), and reinforcement learning agents.As quantum AI matures, we should expect that these and other algorithmic approaches will show a clear advantage when applied to AI But even as quantum libraries, platforms, and tools prove themselves out for these specific challenges, they will still rely on classical AI algorithms and functions within end-to-end machine learning pipelines.For quantum AI to mature into a robust enterprise technology, there will need to be a dominant framework for developing, training, and deploying these applications. Google’s TensorFlow Quantum brings support for a wide range of quantum computing platforms into one of the dominant modeling frameworks used by today’s AI professionals. Developed by Google’s X R&D unit, it enables data scientists to use Python code to develop quantum ML and DL models through standard Keras functions. It also provides a library of quantum circuit simulators and quantum computing primitives that are compatible with existing TensorFlow APIs.Developers can use TensorFlow Quantum for supervised learning on such AI use cases as quantum classification, quantum control, and quantum approximate optimization. They can execute advanced quantum learning tasks such as meta-learning, Hamiltonian learning, and sampling thermal states. They can use the framework to train hybrid quantum/classical models to handle both the discriminative and generative workloads at the heart of the GANs used in deep fakes, 3D printing, and other advanced AI applications.Recognizing that quantum computing is not yet mature enough to process the full range of AI workloads with sufficient accuracy, Google designed the framework to support the many AI use cases with one foot in traditional computing architectures. TensorFlow Quantum enables developers to rapidly prototype ML and DL models that hybridize the execution of quantum and classic processors in parallel on learning tasks. Using the tool, developers can build both classical and quantum datasets, with the classical data natively processed by TensorFlow and the quantum extensions processing quantum data, which consists of both quantum circuits and quantum operators.Google designed TensorFlow Quantum to support advanced research into alternative quantum computing architectures and algorithms for processing ML models. This makes the new offering suitable for computer scientists who are experimenting with different quantum and hybrid processing architectures optimized for ML workloads.To this end, TensorFlow Quantum incorporates In addition to providing a full AI software stack into which quantum processing can now be hybridized, Google is looking to expand the range of more traditional chip architectures on which TensorFlow Quantum can simulate quantum ML. Google also announced plans to expand the range of custom quantum-simulation hardware platforms supported by the tool to include graphics processing units from various vendors as well as its own Tensor Processing Unit AI-accelerator hardware platforms.Google’s latest announcement lands in a fast-moving but still immature quantum computing marketplace. By extending the most popular open source AI development framework, Google will almost certainly catalyze use of TensorFlow Quantum in a wide range of AI-related initiatives.However, TensorFlow Quantum comes into a market that already has several open source quantum-AI development and training tools. Unlike Google’s offering, these rival quantum AI tools come as parts of larger packages of development environments, cloud services, and consulting for standing up full working applications. Here are three full-stack quantum AI offerings:TensorFlow Quantum’s adoption depends on the extent to which these and other quantum AI full-stack vendors incorporate it into their solution portfolios. That seems likely, given the extent to which all these cloud vendors already support TensorFlow in their respective AI stacks.TensorFlow Quantum won’t necessarily have the quantum AI SDK field all to itself going forward. Other open source AI frameworks—most notably, the Facebook-developed PyTorch—are contending with TensorFlow for the hearts and minds of working data scientists. One expects that rival framework to be extended with quantum AI libraries and tools during the coming 12 to 18 months.We can catch a glimpse of the emerging multitool quantum AI industry by considering a pioneering vendor in this regard. Xanadu’s Launched in November 2018, PennyLane is a cross-platform Python library for quantum ML, automatic differentiation, and optimization of hybrid quantum-classical computing platforms. PennyLane enables rapid prototyping and optimization of quantum circuits using existing AI tools, including TensorFlow, PyTorch, and NumPy. It is device-independent, enabling the same quantum circuit model to be run on different software and hardware back ends, including As killer apps and open source frameworks mature, they are sure to catalyze a robust ecosystem of skilled quantum-AI developers who are doing innovative work driving this technology into everyday applications.Increasingly, we’re seeing the growth of a developer ecosystem for quantum AI. Each of the major quantum AI cloud vendors (Google, Microsoft, Amazon Web Services, and IBM) is investing heavily in enlarging the developer community. Vendor initiatives in this regard include the following:",https://www.infoworld.com/article/3546010/quantum-ai-is-still-years-from-enterprise-prime-time.html
Data science during COVID-19: Some reassembly required,"Most likely, the assumptions behind your data science model or the patterns in your data did not survive the coronavirus pandemic. Here’s how to address the challenges of model drift","The enormous impact of the COVID-19 pandemic is obvious. What many still haven’t realized, however, is that the impact on ongoing data science production setups has been dramatic, too. Many of the models used for segmentation or forecasting started to fail when traffic and shopping patterns changed, supply chains were interrupted, and borders were locked down.In short, when people’s behavior changes fundamentally, data science models based on prior behavior patterns will struggle to keep up. Sometimes, data science systems adapt reasonably quickly when the new data starts to represent the new reality. In other cases, the new reality is so fundamentally different that the new data is not sufficient to train a new system. Or worse, the base assumptions built into the system just don’t hold anymore, so the entire process from model creation to production deployment must be revisited.This post describes different scenarios and a few examples of what happens when old data becomes completely outdated, base assumptions are no longer valid, or patterns in the overall system change. I then highlight some of the challenges data science teams face when updating their production system and conclude with a set of recommendations for a robust and future-proof data science setup.The most dramatic scenario is a complete change of the underlying system — one that not only requires an update of the data science process but also a revision of the assumptions that went into its design in the first place. This requires a full new data science creation and productionization cycle: understanding and incorporating business knowledge, exploring data sources (possibly to replace data that doesn’t exist anymore), and selecting and fine-tuning suitable models. Examples include traffic predictions (especially near suddenly closed borders), shopping behavior under more or less stringent lockdowns, and healthcare-related supply chains.A subset of the above is the case where the availability of the data has changed. An illustrative example here is weather predictions, where quite a bit of data is collected by commercial passenger aircraft that are equipped with additional sensors. With the grounding of those aircraft, the volume of available data has been drastically reduced. Because base assumptions about weather systems remain the same (ignoring for a moment that changes in pollution and energy consumption may affect the weather as well) “only” a retraining of the existing models may be sufficient. However, if the missing data represents a significant portion of the information that went into model construction, the data science team would be wise to rerun the model selection and optimization process as well.In many other cases, the base assumptions remain the same. For example, recommendation engines will still work very much the same, but some of the dependencies extracted from the data will change. This is not necessarily very different from, say, a new bestseller entering the charts, but the speed and magnitude of change may be far bigger — as we saw with the sudden spike in demand for health-related supplies. If the data science process has been designed flexibly enough, its built-in change detection mechanism should quickly identify the shift and trigger a retraining of the underlying rules. Of course, that presupposes that change detection was in fact built-in and that the retrained system achieves sufficient quality levels.This brief list is not complete without stressing that many data science systems will continue to work just as they always have. Predictive maintenance is a good example. As long as the usage patterns stay the same, engines will continue to fail in exactly the same ways as before. The important question for the data science team is: Are you sure? Is your performance monitoring setup thorough enough that you can be sure you are not losing quality? Do you even know when the performance of your data science system changes?As noted in the first two impact scenarios above, change to your data science system could happen abruptly (when borders are closed from one day to the next, for example) or only gradually over time. Some of the bigger economic impacts will become apparent in customer behavior only over time. For example, in the case of a SaaS business, customers may not cancel their subscriptions overnight but over coming months.One most often encounters two types of production data science setups. There are the older systems that were built, deployed, and have been running for years without any further refinements, and then there are the newer systems that may have been the result of a consulting project, possibly even a modern automated machine learning (AutoML) type of project. In both cases, if you are fortunate, automatic handling of partial model change has been incorporated into the system, so at least some model retraining is handled automatically. However, none of the currently available AutoML tools allow for performance monitoring and automatic retraining, and usually the older, “one shot” projects don’t worry about that either. As a result, you may not even be aware that your data science process has failed.If you are lucky to have a setup where the data science team has made numerous improvements over the years, chances are higher that automatic model drift detection and retraining have been built-in. However, even then (and especially in the case where a complete model change is required) it is far more likely that the system cannot easily be recreated. Unless all of the steps of your data science process are well documented, and the experts who wrote the code are still with the company, it will be difficult to revisit the assumptions and update the process. The only solution may be to start an entirely new project.Obviously, if your data science process was set up by an external consulting team, you don’t have much of a choice other than to bring them back in. If your data science process is the result of an automated machine learning service, you may be able to re-engage that service, but especially in the case of the change in business dynamics, you should expect to be involved quite a bit—similar to the first time you embarked on this project. One side note here: Be skeptical when someone pushes for supercool new methods. In many cases, a new approach is not needed. Rather, one should focus on carefully revisiting the assumptions and data used for the previous data science process. Only in very few cases is this really a “data 0” problem where one tries to learn a new model from very few data points. Even then, one should also explore the option of building on top of the previous models and keeping them involved in some weighted way. Very often, new behavior can be well represented as a mix of previous models with a sprinkle of new data.But if your data science development is done in-house, now is the time an integrative and uniform environment that is 100% backward compatible comes in very handy. In such a platform, the assumptions are modeled and documented in one place, allowing well-informed changes and adjustments to be made much more easily. It’s even better if you can validate, test, and deploy the changes into production from that same environment without the need for manual interaction.Michael Berthold is CEO and co-founder at KNIME, an open source data analytics company. He has more than 25 years of experience in data science, working in academia, most recently as a full professor at Konstanz University (Germany) and previously at University of California (Berkeley) and Carnegie Mellon, and in industry at Intel’s Neural Network Group, Utopy, and Tripos. Michael has published extensively on data analytics, machine learning, and artificial intelligence. Follow Michael on —New Tech Forum provides a venue to explore and discuss emerging enterprise technology in unprecedented depth and breadth. The selection is subjective, based on our pick of the technologies we believe to be important and of greatest interest to InfoWorld readers. InfoWorld does not accept marketing collateral for publication and reserves the right to edit all contributed content. Send all inquiries to ",https://www.infoworld.com/article/3561290/data-science-during-covid-19-some-reassembly-required.html
"Redis 6: A high-speed database, cache, and message broker","Redis is a powerful blend of speed, resilience, scalability, and flexibility, and Redis Enterprise takes it even further","Like many, you might think of Redis as only a Essentially, Redis is a The core Redis data model is key-value, but many different kinds of values are supported: Strings, Lists, Sets, Sorted Sets, Hashes, Streams, HyperLogLogs, and Bitmaps. Redis also supports geospatial indexes with radius queries and streams.To open source Redis, Redis Enterprise adds features for additional speed, reliability, and flexibility, as well as a cloud database as a service. Redis Enterprise scales linearly to hundreds of millions of operations per second, has active-active global distribution with local latency, offers Redis on Flash to support large datasets at the infrastructure cost of a disk-based database, and provides 99.999% uptime based on built-in durability and single-digit-seconds failover.Further, Redis Enterprise extends the core Redis functionality to support any data modeling method with modules such as RediSearch, RedisGraph, RedisJSON, RedisTimeSeries, and RedisAI, and allows operations to be executed across and between modules and core. All this is provided while keeping database latency under one millisecond.What does it mean that Redis can now function as a database, cache, and message broker? And what are the use cases those roles support?CacheThe Redis can function as a Basic replication allows Redis to scale without using the cluster technology of the Redis Enterprise version. Redis replication uses a leader-follower model (also called master-slave), which is asynchronous by default. Clients can force synchronous replication using a WAIT command, but even that doesn’t make Redis consistent across replicas.Redis has server-side Lua scripting, allowing programmers to extend the database without writing C modules or client-side code. Basic Redis transactions allow a client to declare a sequence of commands as a non-interruptible unit, using the MULTI and EXEC commands to define and run the sequence. This is Redis has different levels of on-disk persistence that the user can select. RDB (Redis database file) persistence takes point-in-time snapshots of the database at specified intervals. AOF (append-only file) persistence logs every write operation received by the server. You can use both RDB and AOF persistence for maximum data safety.Redis Sentinel, itself a distributed system, provides high availability for Redis. It does monitoring of the master and replica instances, notification if there is something wrong, and automatic failover if the master stops working. It also serves as a configuration provider for clients.Redis Cluster provides a way to run a Redis installation where data is automatically sharded across multiple Redis nodes. Redis Cluster also provides some degree of availability during partitions, although the cluster will stop operating if the majority of masters become unavailable.As I mentioned earlier, Redis is a key-value store that supports Strings, Lists, Sets, Sorted Sets, Hashes, Streams, HyperLogLogs, and Bitmaps as values. One of the simplest and most common use cases is using integer values as counters. In support of this, INCR (increment), DECR (decrement), and other single operations are atomic, and therefore safe in a multi-client environment. In Redis, when keys are manipulated they will automatically be created if they don’t already exist.The other kinds of value structures also have their own examples in the There are a number of Using a shared-nothing cluster architecture, Redis Enterprise delivers infinite linear scaling without imposing any non-linear overheads in a scaled-out architecture. You can deploy multiple Redis instances on a single cluster node, to take full advantage of multi-core computer architecture. Redis Enterprise has demonstrated scaling to hundreds of millions of operations per second with five nines (99.999%) uptime. Redis Enterprise does automatic re-sharding and rebalancing while maintaining low latency and high throughput for transactional loads.Redis Enterprise offers active-active deployment for globally distributed databases, enabling simultaneous read and write operations on the same dataset across multiple geo-locations. To make that more efficient, Redis Enterprise can use While there is One of the major differences between Redis and Redis Enterprise is that Redis Enterprise Redis on Flash is a Redis Enterprise feature that can drastically reduce the cost of hardware for Redis. Instead of having to pay through the nose for terabytes of RAM or restrict the size of your Redis datasets, you can use Redis on Flash to place frequently accessed hot data in memory and colder values in Flash or persistent memory, such as Intel Optane DC.Redis Enterprise modules include RedisGraph, RedisJSON, RedisTimeSeries, RedisBloom, RediSearch, and RedisGears. All Redis Enterprise modules also work with open source Redis.Redis 6 is a big release, both for the open source version and the Redis Enterprise commercial version. The performance news is the use of threaded I/O, which gives Redis 6 a 2x improvement in speed over Redis 5 (which was no slouch). That carries over into Redis Enterprise, which has additional speed improvements for clusters as described above.The addition of access control lists (ACLs) gives Redis 6 the concept of users, and allows developers to write more secure code. Redis Enterprise 6 builds on ACLs to offer role-based access control (RBAC), which is more convenient for the programmers and DBAs.Redis Enterprise 6.0 adds support for the Streams data type in active-active databases. That allows both high availability and low latency while concurrently reading and writing to and from a real-time stream in multiple data centers in multiple geographic locations.RedisGears is a dynamic framework that enables developers to write and execute functions that implement data flows in Redis. It lets users write Python scripts to run inside Redis, and enables a number of use cases including write-behind (Redis acts as a front-end to a disk-based database), real-time data processing, streaming and event processing, operations that cross data structures and models, and AI-driven transactions.RedisAI is a model serving engine that runs inside Redis. It can perform inference with PyTorch, TensorFlow, and ONNX models. RedisAI can run on CPUs and GPUs, and enables use cases such as fraud detection, anomaly detection, and personalization.You can install Redis by You can install Redis Enterprise on Linux or in Docker containers. The Linux downloads come in the form of binary packages (DEB or RPM depending on the flavor of Linux) and Bash shell scripts for cluster installation. The scripts check for the required four cores and 15 GB of RAM for installation.The fastest way to install Redis Enterprise is not to install it at all, but rather to run it in the Redis Enterprise Cloud. When I tried this myself for review purposes, I initially received a Redis 5 instance; I had to ask for an upgrade to Redis 6.",https://www.infoworld.com/article/3563354/redis-6-a-high-speed-database-cache-and-message-broker.html
When not to use AIops for cloudops,The excitement around AIops means that it’s sometimes being deployed for the wrong reasons.  Here are a few situations where AIops is contraindicated,"Artificial intelligence for IT operation platforms, better known as AIops, is an evolving and expanded use of technologies that for the past several years were categorized as IT operations analytics. The growth of AIops has been clear to anyone watching the market, but if you need some statistics, Gartner reports that by 2022, 40 percent of large enterprises will use AIops tools to support or replace monitoring and service desk tasks, up from 5 percent today.That’s a pretty big jump. However, it’s also an indication that many enterprises may pick AIops tools for the wrong purposes—mistakes that will likely cost millions. Here is what I’m seeing.Using AIops tools to fix bad cloud architectures and deployments.Just like “you can’t fix stupid,” poorly planned architectures need to be corrected before you apply AIops tools and use them properly. AIops tools work on the assumption that the solution's configuration is sound before they can process alarms and resolutions properly. If not done in that order, you’ll just be teaching your AIops system how to correlate gigabytes of data coming from cloud and non-cloud systems, attempting fixes that are unlikely to be successful because they kick off other alerts and triggers.Hoping AIops tools will eliminate people and costs.I’ve seen enterprises invest in AIops tools based on a business case pointing to fewer ops team members needed, and the ability to automate costs out of the ops equation. Although there is a potential to reduce cost and staff much further down the road, AIops requires a great deal of ops expertise. You typically see AIops drive an expansion of the cloudops team, and costs initially rise for at least a few years. You have to invest in efficiency; you can’t remove dollars and expect good outcomes. Using AIops to deliver better security.AIops is one of those tools that needs to be implemented with great care in order to be effective. The market is moving at such speed that mistakes are going to happen; however, with a bit of common sense, you’ll find that AIops will eventually provide the value you’re looking for.       ",https://www.infoworld.com/article/3563908/when-not-to-use-aiops-for-cloudops.html
How PagerDuty helps customer service and IT teams improve responses,"PagerDuty uses machine learning to anticipate issues and dramatically accelerate responses, so problems are addressed before they impact customers","Predicting the outcome of the NCAA men’s Division I basketball tournament — an event where upsets are celebrated wildly and the outcome is notoriously difficult to foresee — is nearly as competitive as the tournament itself. For years, Warren Buffet held a contest offering a billion dollars for a perfect bracket, and nobody even came close. Speaking of unpredictability, just as fans were getting ready to make their picks for this year’s tournament, all major public sporting events were canceled. Who could have predicted Even though we can’t see the future, a deep understanding of variables does enable people to make better predictions and gain an edge over the competition. Picking winners by their school mascot may work every once in a while, but an in-depth study of the best teams, coaches, and athletes is a much more effective strategy.Likewise, customer service, devops, and IT issues are inherently unpredictable. It’s impossible for companies to know in advance when operational problems will arise, product defects will surface, or communications will go askew. Solutions driven by AI and machine learning can help teams improve their odds. These products can dramatically accelerate responses to issues, so problems are prevented or resolved before most customers encounter them. Companies can get thousands of alerts per minute when a problem arises within their digital app or service — a broken cart for an ecommerce website, for example — which is neither useful nor actionable for human interpreters to tackle. The overwhelming amount of noise simply leads to lost signals and many more contacts between customers and service teams before underlying problems can be addressed.Predictive solutions for customer services are built on understanding the drivers behind the signals. Quickly identifying patterns helps companies stay ahead of the curve. Machine learning tools free up a lot of cycles for response teams by cutting through the noise, rather than distracting them over and over again with alerts and information that may not be useful.When teams use machine learning in this way, they can boil down the signals to uncover the actual incidents that are driving the unmanageable number of alerts. Instead of scrambling to put out many small fires, they can see the big picture of where the problems actually lie and be more intelligent and informed in tackling a smaller group of larger issues.Predictive processes must be conducted in real-time if they are going to help companies get ahead of the issues for the majority of customers. Developing problems that threaten to impact customers do not allow time to be paused for reflection or deliberation.The higher-level need for predictive customer and IT services is in training algorithms to recognize which alerts belong to which incidents. At PagerDuty, our main goal is to help companies identify issues before they cause problems within digital systems, and predict what may go wrong in the future so companies can get ahead of it. We use machine learning to group alerts together so teams can see the full scale of the issues and know precisely how to solve them.For example, multiple teams may each be working on individual complaints without understanding that they are all elements of a single issue. Insights from PagerDuty’s platform solve that problem and get everyone on the same page. Meanwhile, as responders are assigned specific issues to fix, the platform triages messages to each individual so they are not overwhelmed with issues outside of the one they are addressing.This is important because most systems don’t operate in isolation where a point failure in one place is the same as a point failure somewhere else. When problems arise, companies use PagerDuty to help find the origin point for cascading issues to try to prevent catastrophic failure. When teams can be more predictive and preventive, they gain a higher-level view of the problems and learn where their efforts will have the most impact.A structure that helps teams identify and solve problems quickly also gives much greater visibility to every level of the organization. Managers and directors can have better insights about how to deploy teams. Leaders who may have to explain problems or downtime to customers are likewise armed with information and a clear path forward.Giving companies greater predictive and preventive power for customer service and IT starts with grouping issues in a way that helps identify underlying causes of digital issues. That grouping begins with the supposition that if two messages have similar text, then those messages are fundamentally similar. Although this is reasonable in theory, knowing whether those messages are truly similar is a fuzzy concept.At PagerDuty, the most impactful solution has been to apply a parser that takes messages and transforms them into less refined language. This process boils down the words in order to surface specific elements within the message.The system locates unique identifiers like dates, times, customer IDs, or websites with IDs inside that would only be issued in the context of customer messages and reports. These identifiers are generally unimportant to the parser in terms of content. The program simply identifies that they are present within the body of the message.After this overall blurring, the words and identifiers within each message can then be grouped together. This is where PagerDuty’s platform examines the incoming signals and determines the full extent to which messages share groups of words.This step is accomplished by vectorizing, which is the process of turning each of these series of words into a representative sequence of numbers. But it is still an imperfect system. Every sentence produces a vector representation, of course, but each vector could conceivably come from several different sentences. Usually, there is enough information to determine when sentences have the same information. But PagerDuty’s software engineers still have to account for the fact that there are many ways that a vector could have been put together.Once the system can identify a group of messages that have the same vectors, they are bundled together. These groups essentially have the same content. Their identifiers indicate they are full of all the same terms.For example, a company usually learns that something is going wrong when it is suddenly flooded with reports and messages. Most of these will be machine-generated, some with custom templates and some even written by a person. Without some sort of grouping, teams have no way of seeing a higher-level view of the circumstances. They could build a grouping tool, but that would require a serious investment of time and effort, all while more incident reports pile up.Likewise, because so many of the messages have different content, simply grouping messages only when they are identical doesn't do much to diminish the volume of issues. Using AI to discern similarities lets the group accumulate relevant information over time. Instead of thousands of individual problems, each represented by a report or message, grouping alerts this way surfaces just a few core issues that are the source of other problems.At that point, the system has enabled the response teams to become both predictive and preventive. It becomes much easier to find the biggest issues and fix the underlying causes that will prevent future problems. Prioritizing a little engineering work on core issues causes the incident load to drop dramatically, all from basic AI-driven grouping.In theory, this should be a very reliable process. Once messages are parsed, identified, and vectorized, it should be easy for the system to group them together as similar. They are all textually related and the vectors let the platform measure the strength of the relatedness.In practice, of course, it’s not always so simple. The flexibility of language means that the system quite often gets it wrong. This is why PagerDuty builds ample and powerful feedback systems into our products.When end users provide feedback to the system, they are giving us new data points to help perfect the process. This is usually an acknowledgment that, yes, A and B look like they should be related. However, the human context of the message reveals that they don’t have much to do with each other.PagerDuty’s feedback system gives greater weight to messages that have been positively correlated because they share terms, but then human feedback reveals they are not similar. This evaluation and modification could be accomplished in software through a very large reinforcement learning system, but for the user it is a simple evaluation whether terms and messages should or should not be together.  Customers, of course, do not need to see the nuts and bolts of how this works. The customer service and IT teams should have simple tools to provide feedback that will delineate which terms don’t match.On a higher level, PagerDuty’s feedback systems give users broad options for merging and separating groups of terms within the alerts. This is simply grabbing items and moving them in or out of a group; in essence indicating that certain items belong with each other, but another does not match.Another less sophisticated but equally powerful product may only require literal thumbs-up and thumbs-down buttons. The user essentially approves of a match or indicates a flaw in the process.Anything can and will happen to frustrate and disappoint customers, as anyone who has worked in customer service will tell you. Improving your odds in those unpredictable circumstances requires learning, understanding and solving problems as quickly as they emerge. The foremost integrated Chris Bonnell is principal data scientist at —New Tech Forum provides a venue to explore and discuss emerging enterprise technology in unprecedented depth and breadth. The selection is subjective, based on our pick of the technologies we believe to be important and of greatest interest to InfoWorld readers. InfoWorld does not accept marketing collateral for publication and reserves the right to edit all contributed content. Send all inquiries to ",https://www.infoworld.com/article/3544929/how-pagerduty-helps-customer-service-and-it-teams-improve-responses.html
Spell machine learning platform goes on-prem,"An end-to-end machine learning platform designed for ease of use, Spell now offers incarnations for both public cloud and data center deployment","SpellSpell was founded by Spell emphasizes ease of use. For example, hyperparameter optimization for an experiment is a high-level, one-command function. Nor must users do much to configure the infrastructure; Spell detects what hardware is available and orchestrates to suit. Spell also organizes experiment assets, so both experiments and their data can be versioned and check-pointed as part of the development process.Spell originally ran only in the cloud; there’s been no “behind-the-firewall” deployment until now. Spell For Private Machines allows developers to run the platform on their own hardware. Both on-prem and cloud resources can be mixed and matched as needed. For instance, a prototype version of a project could be created on local hardware, then scaled out to an AWS instance for production deployment.Much of Spell’s workflow is already designed to feel as if it runs locally, and to complement existing workflows. Python tools for Spell work can be set up with ",https://www.infoworld.com/article/3541757/spell-machine-learning-platform-goes-on-prem.html
AI is now a C-suite imperative,Survey from Appen finds that C-level involvement in AI initiatives has jumped significantly,"Executive involvement in enterprise artificial intelligence (AI) initiatives is growing rapidly and more emphasis is being placed on high-quality training data. Both C-suite ownership of AI and budgets over $500K nearly doubled in 2020 due to the COVID-19 pandemic serving as a catalyst for accelerated AI initiatives. A key lesson learned from the pandemic is that businesses need to be ready for anything that requires a high level of business agility. It’s Darwinism at its finest as businesses that can adapt to market trends faster than their competition can become market leaders and maintain that position. Those that can’t do this will fade into obscurity with many going away.But how do business leaders even know what decisions to make? There is a massive amount of data to be analyzed and people can’t process the information fast enough to find those key insights that drive business change. Machines can work at infinitely higher speed and the pressure on the C-suite has never been higher. Now the execs are turning to AI to help them make the best decisions in as short a time as possible.These findings come from Appen Limited’s annual The report uncovered an important change. The C-suite is now more engaged in AI initiatives than ever before, with a whopping 71 percent of organizations reporting executive involvement. In comparison, only 39 percent of executives owned AI initiatives in 2019. CTOs made up 42 percent of the 71 percent of C-suite AI ownership, which partially explains why AI budgets are increasing in 2020. COVID-19 may be temporary but don’t expect AI to be. I believe it will be the biggest driver of business change since the rise of the Internet.Executives see AI as invaluable to their business success. This is true for companies of all sizes across different industries. For 27 percent of survey respondents, enterprise AI budgets have exceeded $1M, while 10 percent said their AI budget is more than $5M. These numbers are expected to continue rising steeply as businesses adopt AI on a global scale.With increased C-suite visibility, businesses are focusing more on risk management, governance, and ethics as key aspects of rolling out AI initiatives globally or to their full user base. As companies start using AI to supplement human capabilities, responsible use of AI must be part of the process to ensure fairness, privacy, transparency, and security and avoid inappropriate uses of data.  Although businesses believe responsible AI is important to their success, only 25 percent view unbiased AI as mission-critical. Half of the respondents either don’t see it as a major issue or are just starting to think about it. The findings indicate businesses must take a more proactive approach toward responsible AI. Simply having accurate data or algorithms isn’t enough. AI governance with clear ethical standards is also necessary.Another big challenge for businesses is data management. Three out of four companies surveyed by Appen said they update their AI models at least quarterly. For 40 percent of the respondents, lack of data or data management are the leading roadblocks to effectively utilizing AI in the enterprise.Most respondents (93 percent) expressed the need for high-quality training data, as businesses continue to deal with more data types and complex data compared to previous years. The report revealed many businesses are still behind on AI adoption, especially when it comes to training data. For this reason, company leaders are going beyond in-house resources and turning to third-party providers to help carry out AI deployments.Data management and quality comes up in almost every AI discussion I have with business leaders. Companies have massive amounts of data — more than ever before and it continues to grow exponentially but much of the data resides in silos and is in a myriad of different formats. In data sciences, there’s an axiom that states “good data leads to good insights.” The reverse holds true too, as bad data will lead to bad insights and partial data will lead to partial insights. Getting a handle on data and data quality needs to be as important as the AI initiatives themselves.In 2020, four times as many business and technology decision-makers reported using cloud machine learning providers, including Microsoft Azure (49 percent), Google Cloud (36 percent), IBM Watson (31 percent), AWS (25 percent), and Salesforce Einstein (17 percent). Each of these providers saw double-digit adoption of cloud machine learning tools in 2020 versus 2019, attributing the surge to businesses looking for solutions that can scale as their AI initiatives grow in complexity.Despite dealing with a global pandemic, the majority (70 percent) of businesses surveyed during those months did not expect COVID-19 to negatively impact AI strategies. In fact, nearly half of businesses have fast-tracked their AI strategies, with 20 percent reporting significant acceleration. Only 9 percent of businesses anticipated substantial delays.Increased C-suite investment in enterprise AI is an indication that businesses are choosing to spend money on key initiatives even in a time of crisis. Clearly, these companies view AI-driven agility as the key to both short term survival and long-term leadership. With the right tools and strategies in place, businesses can access clean, high-quality, ethical data to successfully implement AI across the enterprise.",https://www.infoworld.com/article/3563885/ai-is-now-a-c-suite-imperative.html
"How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’","Only a common public health infrastructure of AI, cloud computing, streaming, and the Internet of Things can marshal our data against pandemics","Pandemics are shocks to communities throughout the world. Each community’s response emerges from the countless changes that individuals make in their daily lives to protect themselves while trying to maintain a semblance of normality.Grassroots responses often emerge first in such crises, but they may not be the most effective approach for slowing the contagion’s spread. From a technological standpoint, solutions invariably involve various blends of remote collaboration, contactless transactions, and replacement of manual processes with automated, robotic, and other human-free processes.When a contagion is raging, grassroots responses can be counterproductive if everybody’s operating at cross-purposes. Lack of central coordination can confuse the situation for everybody, stoking a panic-driven Going forward, we can expect to see more data-driven, top-down orchestration of pandemic preparedness and remediation among public, private, and nonprofit organizations. China’s experience is instructive in this regard. Though the outbreak’s inception in Wuhan was less than half a year ago, the country has responded rapidly with a top-down, nationwide approach to manage the crisis. Chinese authorities are orchestrating vast resources to save lives, control the spread of infection, and guide individuals for testing, treatment, and quarantining.In contrast, the United States and other nations seem to be responding to the emergency in a chaotic, bottom-up fashion. The key elements in China’s response are impressive. Leveraging sophisticated data analytics and other digital tools, it has responded to COVID-19 through:Likewise, neighboring Putting this vision in place in the post-pandemic world will require a pervasive infrastructure of AI, cloud computing, streaming, and the Internet of Things. Managed as a common public health infrastructure, these capabilities will allow humanity to close ranks against the dread diseases that threaten us all.",https://www.infoworld.com/article/3543770/how-data-analysis-ai-and-iot-will-shape-the-post-pandemic-new-normal.html
How to move data science into production,"With new Integrated Deployment extensions, data scientists can capture entire KNIME workflows for automatic deployment to production or reuse","Deploying data science into production is still a big challenge. Not only does the deployed data science need to be updated frequently but available data sources and types change rapidly, as do the methods available for their analysis. This continuous growth of possibilities makes it very limiting to rely on carefully designed and agreed-upon standards or work solely within the framework of proprietary tools.KNIME has always focused on delivering an open platform, integrating the latest data science developments by either adding our own extensions or providing wrappers around new data sources and tools. This allows data scientists to access and combine all available data repositories and apply their preferred tools, unlimited by a specific software supplier’s preferences. When using KNIME workflows for production, access to the same data sources and algorithms has always been available, of course. Just like many other tools, however, transitioning from data science creation to data science production involved some intermediate steps.In this post, we are describing a recent addition to the KNIME workflow engine that allows the parts needed for production to be captured directly within the data science creation workflow, making deployment fully automatic while still allowing every module to be used that is available during data science creation.At first glance, putting data science in production seems trivial: Just run it on the production server or chosen device! But on closer examination, it becomes clear that what was built during data science creation is not what is being put into production.I like to compare this to the chef of a Michelin star restaurant who designs recipes in his experimental kitchen. The path to the perfect recipe involves experimenting with new ingredients and optimizing parameters: quantities, cooking times, etc. Only when satisfied, are the final results — the list of ingredients, quantities, procedure to prepare the dish — put into writing as a recipe. This recipe is what is moved “into production,” i.e., made available to the millions of cooks at home that bought the book.This is very similar to coming up with a solution to a data science problem. During data science creation, different data sources are investigated; that data is blended, aggregated, and transformed; then various models (or even combinations of models) with many possible parameter settings are tried out and optimized. What we put into production is not all of that experimentation and parameter/model optimization — but the combination of chosen data transformations together with the final best (set of) learned models.This still sounds easy, but this is where the gap is usually biggest. Most tools allow only a subset of possible models to be exported; many even ignore the preprocessing completely. All too often what is exported is not even ready to use but is only a model representation or a library that needs to be consumed or wrapped into yet another tool before it can be put into production. As a result, the data scientists or model operations team needs to add the selected data blending and transformations manually, bundle this with the model library, and wrap all of that into another application so it can be put into production as a ready-to-consume service or application. Lots of details get lost in translation.For our Michelin chef above, this manual translation is not a huge issue. She only creates or updates recipes every other year and can spend a day translating the results of her experimentation into a recipe that works in a typical kitchen at home. For our data science team, this is a much bigger problem: They want to be able to update models, deploy new tools, and use new data sources whenever needed, which could easily be on a daily or even hourly basis. Adding manual steps in between not only slows this process to a crawl but also adds many additional sources of error.The diagram below shows how data science creation and productionization intertwine. This is inspired by the classic CRISP-DM cycle but puts stronger emphasis on the continuous nature of data science deployment and the requirement for constant monitoring, automatic updating, and feedback from the business side for continuous improvements and optimizations. It also distinguishes more clearly between the two different activities: creating data science and putting the resulting data science process into production.Often, when people talk about “end-to-end data science,” they really only refer to the cycle on the left: an integrated approach covering everything from data ingestion, transforming, and modeling to writing out some sort of a model (with the caveats described above). Actually consuming the model already requires other environments, and when it comes to continued monitoring and updating of the model, the tool landscape becomes even more fragmented. Maintenance and optimization are, in many cases, very infrequent and heavily manual tasks as well. On a side note: We avoid the term “model ops” purposely here because the data science production process (the part that’s moved into “operations”) consists of much more than just a model.Integrated deployment removes the gap between data science creation and data science production by enabling the data scientist to model both creation as well as production within the same environment by capturing the parts of the process that are needed for deployment. As a result, whenever changes are made in data science creation, these changes are automatically reflected in the deployed extract as well. This is conceptually simple but surprisingly difficult in reality.If the data science environment is a programming or scripting language, then you have to be painfully detailed about creating suitable subroutines for every aspect of the overall process that could be useful for deployment — also making sure that the required parameters are properly passed between the two code bases. In effect, you have to write two programs at the same time, ensuring that all dependencies between the two are always observed. It is easy to miss a little piece of data transformation or a parameter that is needed to properly apply the model.Using a visual data science environment can make this more intuitive. The new Integrated Deployment node extensions from KNIME allow those pieces of the workflow that will also be needed in deployment to be framed or captured. The reason this is so simple is that those pieces are naturally a part of the creation workflow. This is because first, the exact same transformation pieces are needed during model training, and second, evaluation of the models is needed during fine tuning. The following image shows a very simple example of what this looks like in practice:The purple boxes capture the parts of the data science creation process that are also needed for deployment. Instead of having to copy them or having to go through an explicit “export model” step, now we simply add Capture-Start/Capture-End nodes to frame the relevant pieces and use the Workflow-Combiner to put the pieces together. The resulting, automatically created workflow is shown below:The Workflow-Writer nodes come in different shapes that are useful for all possible ways of deployment. They do just what their name implies: write out the workflow for someone else to use as a starting point. But more powerful is the ability to use Workflow-Deploy nodes that automatically upload the resulting workflow as a REST service or as an analytical application to KNIME Server or deploy it as a container — all possible by using the appropriate Workflow-Deploy node.The purpose of this article is not to describe the technical aspects in great detail. Still, it is important to point out that this capture and deploy mechanism works for all nodes in KNIME — nodes that provide access to native data transformation and modeling techniques as well as nodes that wrap other libraries such as TensorFlow, R, Python, Weka, Spark, and all of the other third-party extensions provided by KNIME, the community, or the partner network.With the new Integrated Deployment extensions, KNIME workflows turn into a complete data science creation and productionization environment. Data scientists building workflows to experiment with built-in or wrapped techniques can capture the workflow for direct deployment within that same workflow. For the first time, this enables instantaneous deployment of the complete data science process directly from the environment used to create that process.Michael Berthold is CEO and co-founder at KNIME, an open source data analytics company. He has more than 25 years of experience in data science, working in academia, most recently as a full professor at Konstanz University (Germany) and previously at University of California (Berkeley) and Carnegie Mellon, and in industry at Intel’s Neural Network Group, Utopy, and Tripos. Michael has published extensively on data analytics, machine learning, and artificial intelligence. Follow Michael on —New Tech Forum provides a venue to explore and discuss emerging enterprise technology in unprecedented depth and breadth. The selection is subjective, based on our pick of the technologies we believe to be important and of greatest interest to InfoWorld readers. InfoWorld does not accept marketing collateral for publication and reserves the right to edit all contributed content. Send all inquiries to ",https://www.infoworld.com/article/3541230/how-to-move-data-science-into-production.html
Apache Spark 3.0 adds Nvidia GPU support for machine learning,The next major release of the in-memory data processing framework will support GPU-accelerated functions courtesy of Nvidia RAPIDS,"Apache Spark, the The GPU acceleration components, provided by Nvidia, are designed to complement all phases of Spark applications including ETL operations, machine learning training, and inference serving.Nvidia’s Spark contributions draw on the Spark 3.0 speedups don’t come solely from GPU acceleration. Spark 3.0 also reaps performance gains by minimizing data movement to and from GPUs. When data does need to be moved across a cluster, the According to Nvidia, a preview release of Spark 3.0 running on the No firm date has been given for general availability of Spark 3.0. You can ",https://www.infoworld.com/article/3543319/apache-spark-30-adds-nvidia-gpu-support-for-machine-learning.html
6 Python libraries for parallel processing,Want to distribute that heavy Python workload across multiple CPUs or a compute cluster? These frameworks can make it happen,"PythonAnd while you can use the Python does include a native way to run a Python workload across multiple CPUs. The Sometimes the job calls for distributing work not only across Developed by a team of researchers at the University of California, Berkeley, Ray’s syntax is minimal, so you don’t need to rework existing apps extensively to parallelize them. The Ray even includes its own built-in cluster manager, which can automatically spin up nodes as needed on local hardware or popular cloud computing platforms.From the outside, Dask works in two basic ways. The first is by way of parallelized data structures — essentially, Dask’s own versions of NumPy arrays, lists, or Pandas DataFrames. Swap in the Dask versions of those constructions for their defaults, and Dask will automatically spread their execution across your cluster. This typically involves little more than changing the name of an import, but may sometimes require rewriting to work completely.The second way is through Dask’s low-level parallelization mechanisms, including function decorators, that parcel out jobs across nodes and return results synchronously (“immediate” mode) or asynchronously (“lazy”). Both modes can be mixed as needed, too.One key difference between Dask and Ray is the scheduling mechanism. Dask uses a centralized scheduler that handles all tasks for a cluster. Ray is decentralized, meaning each machine runs its own scheduler, so any issues with a scheduled task are handled at the level of the individual machine, not the whole cluster.Dask also offers an advanced and still experimental feature called “actors.” An actor is an object that points to a job on another Dask node. This way, a job that requires a lot of local state can run in-place and be called remotely by other nodes, so the state for the job doesn’t have to be replicated. Ray lacks anything like Dask’s actor model to support more sophisticated job distribution.DispyDispy syntax somewhat resembles Pandaral·lelNote that while Pandaral·lel does run on Windows, it will run only from Python sessions launched in the Windows Subsystem for Linux. MacOS and Linux users can run Pandaral·lel as-is. IpyparallelIpyparallel supports many approaches to parallelizing code. On the simple end, there’s Jupyter notebooks support “magic commands” for actions only possible in a notebook environment. Ipyparallel adds a few magic commands of its own. For example, you can prefix any Python statement with JoblibJoblib syntax for parallelizing work is simple enough—it amounts to a decorator that can be used to split jobs across processors, or to cache results. Parallel jobs can use threads or processes.Joblib includes a transparent disk cache for Python objects created by compute jobs. This cache not only helps Joblib avoid repeating work, as noted above, but can also be used to suspend and resume long-running jobs, or pick up where a job left off after a crash. The cache is also intelligently optimized for large objects like NumPy arrays. Regions of data can be shared in-memory between processes on the same system by using One thing Joblib does not offer is a way to distribute jobs across multiple separate computers. In theory it’s possible to use Joblib’s pipeline to do this, but it’s probably easier to use another framework that supports it natively. ",https://www.infoworld.com/article/3542595/6-python-libraries-for-parallel-processing.html
How AIOps improves application monitoring,Devops and site reliability engineers are vital to keep applications functioning. AIOps boosts effectiveness another notch,"IT operation teams use many tools to monitor, diagnose, and resolve system and application performance issues. In a recent survey of 1,300 IT professionals on That’s a lot of technology just to keep the lights on and provide the data required to monitor, alert, research, and resolve application incidents.Monitoring tools are not one size fits all, especially for organizations running mission-critical applications in multicloud environments. As organizations invest in mobile apps, microservices, AIOps platforms aim to simplify this landscape of monitoring tools. AIOps helps organizations that require high application service levels better manage the complexity of their monitoring tools and IT operational workflows. As the name suggests, AIOps brings machine learning and automation capabilities to the IT operations domain. These technologies aim to resolve incidents faster, identify operational trends that impact performance, and simplify the procedures required to resolve issues.AIOps is an emerging platform. In the survey, 42 percent of respondents either had never heard of AIOps or had thought that applying machine learning to operations was “not a thing.” Only 4 percent are using an AIOps tool in production today. Although AIOps is an emerging platform, there’s a solid business case for many organizations to consider it.More businesses today rely on applications to serve customers and run operations. That drives higher requirements and expectations on the reliability, performance, and security of the applications.It also fuels demand for application development teams to build new applications and enhance them more frequently. The job responsibility of maintaining application service levels has also broadened during the past decade.Once upon a time, organizations staffed the NOC (network operations center) as the front line of defense. If you ever walked into a NOC, you would likely see dozens of computer monitors with warning lights and trend visuals to help the staff pinpoint issues—ideally before an end-user experienced one and opened tickets.Business and IT leaders began changing this model by introducing devops practices and site reliability engineers. Devops changes the IT department’s culture by establishing a collective responsibility to enable frequent deployments and better But devops practices also require a shared operational responsibilityMany IT organizations also hire SREs (Maturing devops practices and hiring site reliability engineersHow can AIOps improve the status quo? AIOps platforms typically have the following architecture components and capabilities:What differentiates AIOps from other IT operational platforms is the ability to aggregate data easily, leverage machine learning to find problems, and use automation as a tool to resolve them. AIOps doesn’t replace the existing monitoring tools. It integrates with them so that more people in the IT department have improved visibility to problems without the complexity of learning and using multiple monitoring tools.Similarly, AIOps platforms typically don’t replace existing IT service management, workflow, agile, and other communication tools. Instead, they are a central platform to interface with them while alerting and resolving an incident.Imagine your e-commerce application experiences slow performance when users try to complete a purchase. The first indicator that starts to send out alerts is the shopping cart abandonment rate.The e-commerce leader quickly opens a ticket about the issue in Cherwell’s mobile interface, but the IT team has already been alerted to the problem. As more users try to make purchases, the underlying Web servers hang and database connections stay open. Alerts from DataDog report these issues, and Splunk reports Java exceptions in the e-commerce application’s log files.Now imagine the NOC responding to this issue. Where should they start, given the number of alerts going off at the same time? The SREs called in to assist must also investigate the different alerts from different tools. Meanwhile, the e-commerce leader is upset because no one responded to her ticket!Here’s how AIOps platforms can potentially address this issue faster and more effectively.First, AIOps sees that multiple alerts are going off, including application alerts. It automatically alerts the SREs, and when one responds, it automatically updates Cherwell that the incident has been answered by an SRE. No one had to manually update any system to send out these communications.Second, the alerts from Cherwell, the e-commerce platform, Splunk, and DataDog are all aggregated and time sequenced. The SRE immediately knows which alert came before the others triggered. That’s incredibly useful because the SRE can quickly see that the Web server hanging and the pooling database connections all started after the Java application exceptions.The AIOps platform’s machine learning capabilities are fairly sophisticated, so in addition to reporting on alerts, it also highlights other outlier operating conditions. In this case, the e-commerce application has many slow outbound connections to a single IP address. There are no alerts or exceptions on this issue, but its timing precedes any of the other alerts.It doesn’t take the SRE much longer to figure out that this is a connection to a third-party service that validates the city, state, and ZIP code of the buyer. This service is clearly having performance issues that are rippling through the entire application.With a root cause identified, the SRE adds a high-severity defect to the e-commerce development team’s Jira backlog, alerting them to the problem. A high-severity issue flags the agile development team to disrupt their sprint and address it. It’s a quick fix to circumvent the impacting service, and it’s easy to test and deploy the change through their Jenkins CI/CD pipeline.The AIOps platform tracks this defect, the deployment, and the drop in all the alerts and keeps the e-commerce leader updated on the progress. Even though the SRE is monitoring the situation, the AIOps platform closes the issue automatically when all the monitors return to normal.Implementing this scenario isn’t trivial, but neither is it science fiction with AIOps platforms.",https://www.infoworld.com/article/3541308/how-aiops-improves-application-monitoring.html
Google unveils TensorFlow tool for making mobile-ready models,TensorFlow Lite Model Maker shrinks TensorFlow models to more efficiently serve predictions on mobile devices,"Google has announced TensorFlow models can be quite large, and serving predictions remotely from beefy hardware capable of handling them isn’t always possible. Google created the TensorFlow Lite model format to make it more efficient to serve predictions locally, but creating a TensorFlow Lite version of a model previously required some work.In a blog postOther TensorFlow Lite tools announced in the same post include a ",https://www.infoworld.com/article/3538913/google-unveils-tensorflow-tool-for-making-mobile-ready-models.html
GPipe and PipeDream: Scaling AI training in every direction,New frameworks Google GPipe and Microsoft PipeDream join Uber Horovod in distributed training for deep learning,"Data science is hard work, not a magical incantation. Whether an AI model performs as advertised depends on how well it’s been trained, and there’s no “one size fits all” approach for training AI models.Scaling is one of the trickiest considerations when training AI models. Training can be especially challenging when a model grows too resource hungry to be processed in its entirety on any single computing platform. A model may have grown so large it exceeds the memory limit of a single processing platform, or an accelerator has required developing special algorithms or infrastructure. Training data sets may grow so huge that training takes an inordinately long time and becomes prohibitively expensive.Scaling can be a piece of cake if we don’t require the model to be particularly good at its assigned task. But as we ramp up the level of inferencing accuracy required, the training process can stretch on longer and chew up ever more resources. Addressing this issue isn’t simply a matter of throwing more powerful hardware at the problem. As with many application workloads, one can’t rely on faster processors alone to sustain linear scaling as AI model complexity grows.Distributed training may be necessary. If the components of a model can be partitioned and distributed to optimized nodes for processing in parallel, the time needed to train a model can be reduced significantly. However, parallelization can itself be a fraught exercise, considering how fragile a construct a statistical model can be.The model may fail spectacularly if some seemingly minor change in the graph—its layers, nodes, connections, weights, hyperparameters, etc.—disrupts the model’s ability to make accurate inferences. Even if we leave the underlying graph intact and attempt to partition the model’s layers into distributed components, we will then need to recombine their results into a cohesive whole.If we’re not careful, that may result in a recombined model that is skewed in some way in the performance of its designated task.Throughout the data science profession, we continue to see innovation in AI model training, with much of it focusing on how to do it efficiently in multiclouds and other distributed environments.In that regard, Google and Microsoft Though different in several respects, GPipe and PipeDream share a common vision for distributed AI model training. This vision involves the need to:What distinguishes these two frameworks is the extent to which they support optimized performance of training workflows for models with sequential layers (which is always more difficult to parallelize) and in more complex target environments, such as multicloud, mesh, and cloud-to-edge scenarios.Google’s GPipe is well suited for fast parallel training of deep neural networks that incorporate multiple sequential layers. It automatically does the following:Microsoft’s PipeDream also exploits model and data parallelism, but it’s more geared to boosting performance of complex AI training workflows in distributed environments. One of the AI training projects in Microsoft Research’s Further details on both frameworks are in their respective research papers: Training is a critical feature of AI’s success, and more AI professionals are distributing these workflows across multiclouds, meshes, and distributed edges.Going forward, Google and Microsoft should align their respective frameworks into an industry consensus approach for distributed AI training. They might want to consider engaging Uber in this regard. The ride sharing company already has a greater claim to the first-to-market distinction in distributed training frameworks. It Scalability should be a core consideration of any and all such frameworks. Right now, Horovod has some useful features in that regard but lacks the keen scaling focus that Google and Microsoft have built into their respective projects. In terms of scalability, Horovod can run on single or multiple GPUs, and even on multiple distributed hosts without code changes. It is capable of batching small operations, automating distributed tuning, and interleaving communication and computation pipelines.Scalability concerns will vary depending on what training scenario you consider. Regardless of which framework becomes dominant—GPipe, PipeDream, Horovod, or something else—it would be good to see industry development of reference workflows for distributed deployment of the following specialized training scenarios:This list doesn’t even begin to hint at the diversity of distributed AI training workflows that will be prevalent in the future. To the extent that we have standard reference frameworks in place in 2020, data scientists will have a strong foundation for carrying the AI revolution forward in every direction.",https://www.infoworld.com/article/3539741/gpipe-and-pipedream-scaling-ai-training-in-every-direction.html
Checking AI bias is a job for the humans,"By pre-processing or post-processing data, or even setting datasets to expire, humans can step in to correct machine learning models","One of the primary problems with artificial intelligence (AI) is the “artificial” part. The other is the “intelligence.” While we like to pretend that we’re setting robotic intelligences free from our human biases and other shortcomings, in reality we often transfer our failings into the AI, one dataset at a time.Hannah Davis, a data scientist, It has become de rigueur to posture how very “data driven” we are, and nowhere more so than in AI, which is completely dependent on data to be of use. One of the wonders of machine learning algorithms, for example, is how fast they can sift through mountains of data to uncover patterns and respond accordingly. Such models, however, must be trained, which is why data scientists tend to congregate around established, high-quality Unfortunately, those datasets aren’t neutral, as Davis points out:See the problem? Machine learning models are only as smart as the datasets that feed them, and those datasets are limited by the people shaping them. This could lead, as Complicating matters further, our own errors and biases are, in turn, shaped by machine learning models. As Time to abandon hope, all we who enter here?Not necessarily. As Davis goes on to suggest, one key thing we can do is to set our datasets to expire:At any given point in time, the people, places, or things that are top of mind will tend to find their way into our datasets. (Davis uses the example of ImageNet, created in 2009, which returns flip phones when “cell phone” is searched.) By setting datasets to expire, we force our models to keep up with society.This calls out another option, Unless we’re careful, Davis warns, “It is easy to accidentally cause harm through something as seemingly simple as collecting and labeling data.” But with extra care, we can gain much of the benefits of AI while minimizing the potential biases and other shortcomings that the machines inherit from us humans.",https://www.infoworld.com/article/3537968/checking-ai-bias-is-a-job-for-the-humans.html
"PyTorch 1.5 adds C++ power, distributed training",The powerful deep learning system for Python now makes it easier to integrate high performance C++ code and train models on multiple machines at once,"PyTorch, the Python framework for PyTorch 1.5 brings a major update to PyTorch’s C++ front end, PyTorch 1.5 also adds a way to bind custom C++ classes to TorchScript and Python. PyTorch 1.5 also drops support for Python 2 and requires Python 3.5 and higher. This is in line with other major Python frameworks, as Finally, PyTorch 1.5 brings a stable version of the With PyTorch 1.5, the RPC framework can be used to build training applications that make use of distributed architectures if they’re available. The RPC framework is designed to minimize the amount of data copying across nodes, so work is always done as close to the data as possible.",https://www.infoworld.com/article/3539275/pytorch-15-adds-c-power-distributed-training.html
AWS unveils open source model server for PyTorch,"Intended to ease production deployments of PyTorch models, TorchServe supports multi-model serving and model versioning for A/B testing","Amazon Web Services (AWS) has unveiled an open source tool, called TorchServe, for serving PyTorch machine learning models. TorchServe is maintained by AWS in partnership with Facebook, which developed PyTorch, and is available as part of the Released on April 21, TorchServe is designed to make it easy to deploy PyTorch models at scale in production environments. Goals include lightweight serving with low latency, and high-performance inference.The key features of TorchServe include:Any deployment environment can be supported by TorchServe, including Kubernetes, Amazon SageMaker, Amazon EKS, and Amazon EC2. TorchServe requires Java 11 on Ubuntu Linux or MacOS. ",https://www.infoworld.com/article/3540415/aws-unveils-open-source-model-server-for-pytorch.html
Review: Amazon SageMaker plays catch-up,"With Studio, Autopilot, and other additions, Amazon SageMaker is now competitive with the machine learning environments available in other clouds","When I Since then, the scope of SageMaker has expanded, augmenting the core notebooks with IDEs (SageMaker Studio) and automated machine learning (SageMaker Autopilot) and adding a bunch of important services to the overall ecosystem, as shown in the diagram below. This ecosystem supports machine learning from preparation through model building, training, and tuning to deployment and management — in other words, end to end.What’s new? Given that I last looked at SageMaker just after it was released, the list is rather long, but let’s start with the most visible services.Other notable improvements include the optional use of spot instances for notebooks to reduce the cost; a new P3dn.24xl instance type that includes eight V100 GPUs; an AWS-optimized TensorFlow framework, which achieves close to linear scalability when training multiple types of neural networks; Amazon Elastic Inference, which can dramatically decrease inference costs; AWS Inferentia, which is a high-performance machine learning inference chip; and new algorithms, both built-in to SageMaker and  available in the AWS Marketplace. In addition, SageMaker Neo compiles deep learning models to run on edge computing devices, and SageMaker RL (not shown on the diagram) provides a managed reinforcement learning service.JupyterLab is the next-generation, web-based user interface for Project Jupyter. SageMaker Studio uses JupyterLab as the basis for an IDE that’s a unified online machine learning and deep learning workstation with collaboration features, experiment management, Git integration, and automatic model generation.The screenshot below shows how to install the SageMaker examples into a SageMaker Studio instance, using a terminal tab and the Git command line. The instructions for doing this are in the README for this example, which is kind of a Catch-22. You can read them by browsing to the Amazon’s Getting Started example contains a notebook called xgboost_customer_churn_studio.ipynb, which was adapted from a The example goes on to run an additional training with an external XGBoost algorithm modified to save debugging information to Amazon S3 and to invoke three debugging rules. This is in what’s called When the trainings are all done, you can compare the results in the Experiments tab.The example then hosts the model using its By the way, XGBoost is only one of the many algorithms built into SageMaker. A full list is shown in the table below — and you can always create your own model.Suppose you don’t know how to do feature engineering and you aren’t very familiar with the different algorithms available for the various machine learning tasks. You can still use SageMaker — just let it run on autopilot. SageMaker Autopilot is capable of handling datasets up to 5 GB.In the screenshot below we are running the The example then picks the best model, uses it to create and host an endpoint, and runs a transform job to add the model predictions to a copy of the test data. Finally, it finds the two notebooks created by the Autopilot job.There is a user interface to the Autopilot results, although it’s not obvious. If you right-click on the automl experiment you can see all the trials with their objective values, as shown below.If you’re lucky, all your data will be labeled, or otherwise annotated, and ready to be used as a training dataset. If not, you can annotate the data manually (the standard joke is that you give the task to your grad students), or you can use a As you can see in the diagram below, Ground Truth can be applied to a number of different tasks. With Ground Truth, you can use workers from either Amazon Mechanical Turk, or a vendor company that you choose, or an internal, private workforce along with machine learning to enable you to create a labeled dataset.Amazon provides Until recently, deploying trained models on edge devices — smartphones and IoT devices, for example — has been difficult. There have been specific solutions, such as TensorFlow Lite for TensorFlow models and TensorRT for Nvidia devices, but According to AWS, Neo can double the performance of models and shrink them enough to run on edge devices with limited amounts of memory.In terms of compute, storage, network transfer, etc., deploying models for production inference often accounts for 90 percent of the cost of deep learning, while the training accounts for only 10 percent of the cost. AWS offers many ways to reduce the cost of inference.One of these is Elastic Inference is supported in Elastic Inference-enabled versions of TensorFlow, Apache MXNet, and PyTorch. To use any other deep learning framework, export your model by using ONNX, and then import your model into MXNet.If you need more than the 32 TFLOPS per accelerator you can get from Elastic Inference, you can use EC2 G4 instances, which have Nvidia T4 GPUs, or EC2 Inf1 instances, which have AWS Inferentia custom accelerator chips. If you need the speed of Inferentia chips, you can use the At this point, the Amazon SageMaker Studio preview is good enough to use for end-to-end machine learning and deep learning: data preparation, model training, model deployment, and model monitoring. While the user experience still leaves a few things to be desired, such as better discovery of functionality, Amazon SageMaker is now competitive with the machine learning environments available in other clouds.—Cost:Platform:",https://www.infoworld.com/article/3534699/review-amazon-sagemaker-plays-catch-up.html
What is Apache Spark? The big data platform that crushed Hadoop,"Fast, flexible, and developer-friendly, Apache Spark is the leading platform for large-scale SQL, batch processing, stream processing, and machine learning","Apache SparkFrom its humble beginnings in the AMPLab at U.C. Berkeley in 2009, At a fundamental level, an Apache Spark application consists of two main components: a Out of the box, Spark can run in a standalone cluster mode that simply requires the Apache Spark framework and a JVM on each machine in your cluster. However, it’s more likely you’ll want to take advantage of a more robust resource or cluster management system to take care of allocating workers on demand for you. In the enterprise, this will normally mean running on If you seek a managed solution, then Apache Spark can be found as part of Apache Spark builds the user’s data processing commands into a It’s worth pointing out that Apache Spark vs. The first advantage is speed. Spark’s in-memory data engine means that it can perform tasks up to one hundred times faster than MapReduce in certain situations, particularly when compared with multi-stage jobs that require the writing of state back out to disk between stages. In essence, MapReduce creates a two-stage execution graph consisting of data mapping and reducing, whereas Apache Spark’s DAG has multiple stages that can be distributed more efficiently. Even Apache Spark jobs where the data cannot be completely contained within memory tend to be around 10 times faster than their MapReduce counterpart.The second advantage is the developer-friendly Spark API. As important as Spark’s speedup is, one could argue that the friendliness of the Spark API is even more important.In comparison to MapReduce and other Apache Hadoop components, the Apache Spark API is very friendly to developers, hiding much of the complexity of a distributed processing engine behind simple method calls. The canonical example of this is how almost 50 lines of MapReduce code to count words in a document can be reduced to just a few lines of Apache Spark (here shown in Scala):By providing bindings to popular languages for data analysis like Python and R, as well as the more enterprise-friendly Java and Scala, Apache Spark allows everybody from application developers to data scientists to harness its scalability and speed in an accessible manner.At the heart of Apache Spark is the concept of the RDDs can be created from simple text files, SQL databases, NoSQL stores (such as Cassandra and MongoDB), Amazon S3 buckets, and much more besides. Much of the Spark Core API is built on this RDD concept, enabling traditional map and reduce functionality, but also providing built-in support for joining data sets, filtering, sampling, and aggregation.Spark runs in a distributed fashion by combining a Originally known as Shark, Alongside standard SQL support, Spark SQL provides a standard interface for reading from and writing to other datastores including JSON, HDFS, Apache Hive, JDBC, Apache ORC, and Apache Parquet, all of which are supported out of the box. Other popular stores—Apache Cassandra, MongoDB, Apache HBase, and many others—can be used by pulling in separate connectors from the Selecting some columns from a dataframe is as simple as this line:Using the SQL interface, we register the dataframe as a temporary table, after which we can issue SQL queries against it:Behind the scenes, Apache Spark uses a query optimizer called Spark 2.4 introduced a set of Apache Spark also bundles libraries for applying machine learning and graph analysis techniques to data at scale. Note that while Spark MLlib covers basic machine learning including classification, regression, clustering, and filtering, it does not include facilities for modeling and training deep neural networks (for details Spark GraphXSpark Streaming Spark Streaming extended the Apache Spark concept of batch processing into streaming by breaking the stream down into a continuous series of microbatches, which could then be manipulated using the Apache Spark API. In this way, code in batch and streaming operations can share (mostly) the same code, running on the same framework, thus reducing both developer and operator overhead. Everybody wins.A criticism of the Spark Streaming approach is that microbatching, in scenarios where a low-latency response to incoming data is required, may not be able to match the performance of other streaming-capable frameworks like Apache Storm, Structured Streaming Structured Streaming originally relied on Spark Streaming’s microbatching scheme of handling streaming data. But in Spark 2.3, the Apache Spark team added a low-latency Structured Streaming is the future of streaming applications with the platform, so if you’re building a new streaming application, you should use Structured Streaming. The legacy Spark Streaming APIs will continue to be supported, but the project recommends porting over to Structured Streaming, as the new method makes writing and maintaining streaming code a lot more bearable.Apache Spark supports deep learning via Ready to dive in and learn Apache Spark? We highly recommend Evan Heitman’s If you’re looking for some Apache Spark examples to give you a sense of what the platform can do and how it does it, check out Need to go deeper? DZone has what it modestly refers to as ",https://www.infoworld.com/article/3236869/what-is-apache-spark-the-big-data-platform-that-crushed-hadoop.html
Kaggle calls data scientists to action on COVID-19,"In newest challenge, Kaggle asks AI researchers to apply machine learning tools and techniques to answering questions about COVID-19","Kaggle, an online community for data scientists and a platform for data science competitions, has unveiled a new and timely bounty-paying challenge: the COVID-19 Open Research Dataset Challenge, or CORD-19 asks AI and machine learning researchers to develop text and data mining tools to analyze a dataset comprising tens of thousands of articles on virology and infectious disease. The goal is to help provide answers for The The world isn’t lacking for research about COVID-19. Kaggle’s dataset contains “over 29,000 scholarly articles, including over 13,000 with full text, about COVID-19, SARS-CoV-2, and releated coronaviruses,” according to the challenge introduction. But there’s little time to paw manually through those haystacks of research for the needles we need, so Kaggle is encouraging the use of machine learning techniques like The CORD-19 tasks revolve around common questions about COVID-19. Each of the high-level tasks (e.g., What do we know about COVID-19 risk factors?) includes a number of subtasks (e.g., What populations are more susceptible? What roles do smoking or pre-existing pulmonary diseases play?).Other COVID-19-related datasets are also available on Kaggle. These include Previous Kaggle challenges related to medical science featured projects with longer and less urgent time frames, such as ",https://www.infoworld.com/article/3533269/kaggle-calls-data-scientists-to-action-on-covid-19.html
TensorFlow deepens its advantages in the AI modeling wars,"Despite complaints about its complexity, TensorFlow supports every AI development, training, and deployment scenario you can imagine","TensorFlowIn the As the decade proceeds, the differences between these frameworks will diminish as data scientists and other users value feature parity over strong functional differentiation. Nevertheless, TensorFlow remains by far the top AI modeling framework, not just in adoption and maturity, but in terms of the sheer depth and breadth of the stack in supporting every conceivable AI development, training, and deployment scenario. PyTorch, though strong for 80 percent of the core AI, deep learning, and machine learning challenges, has a long way to go before it arrives at feature parity.Last week, Google’s TensorFlow team distanced its stack further from PyTorch. It held its yearly What follows is my discussion of the announcements organized into three broad categories: enhancements to the core TensorFlow and its ecosystem, new TensorFlow devops tools, and new TensorFlow add-ons for specialized AI development challenges.In terms of enhancements to the core open source code, the team announced Ensuring full support for cross-device compilation of models built in TensorFlow, the team announced TensorFlow Runtime, which will ship with the core open source code and will ensure that ML models work on different devices and platforms that implement the Multi-Level Intermediate Representation standard, which is supported by 95 percent of hardware manufacturers.Making sure that models developed in TensorFlow can run on Google’s own neural network processor, the team announced that the previous version, TensorFlow 2.1, also supports Cloud Tensor processing units.Many enterprise AI developers use TensorFlow to build, train, and deploy deep learning, ML, and NLP models within complex devops workflows. To address these requirements, the team announced new capabilities to accelerate team productivity across the TensorFlow ecosystem:TensorFlow users are working in an astonishing range of sophisticated projects that push AI’s boundaries from every direction. To support these requirements, the team announced the following new tools, extensions, and add-ons that plug into TensorFlow’s ecosystem.That’s an extraordinarily deep and broad set of announcements, reflecting the deep pockets and vast brainpower that Google and its parent Alphabet are investing in the TensorFlow ecosystem. Taken together, they may not stanch grumbling among AI developers about TensorFlow, which many deem confusing, hard to use, bloated, slow, inefficient, and excessively complex. In fact, the glut of new capabilities may exacerbate those issues to the point where they cause defections to simpler AI modeling frameworks.Nevertheless, TensorFlow is still very much the framework to beat from the perspective of professional data scientists who are building mission-critical AI projects for production environments. This week’s announcements have made its advantages for heavy-hitting AI development even more pronounced.",https://www.infoworld.com/article/3534474/tensorflow-deepens-its-advantages-in-the-ai-modeling-wars.html
Explaining machine learning models to the business,How to create summaries of machine learning system decisions that business decision makers can understand,"Explainable machine learning is a sub-discipline of artificial intelligence (AI) and machine learning that attempts to summarize how machine learning systems make decisions. Summarizing how machine learning systems make decisions can be helpful for a lot of reasons, like finding data-driven insights, uncovering problems in machine learning systems, facilitating regulatory compliance, and enabling users to appeal — or operators to override — inevitable wrong decisions.Of course all that sounds great, but explainable machine learning is not yet a perfect science. The reality is there are two major issues with explainable machine learning to keep in mind:For issue 1, I’m going to assume that you want to use one of the many kinds of “glass-box” accurate and interpretable machine learning models available today, like monotonic gradient boosting machines in the open source frameworks h2o-3, LightGBM, and XGBoost.This article is divided into two main parts. The first part addresses explainable machine learning summaries for a machine learning system and an entire dataset (i.e. “global” explanations). The second part of the article discusses summaries for machine learning system decisions about specific people in a dataset (i.e. “local” explanations). Also, I’ll be using a straightforward example problem about predicting credit card payments to present concrete examples.Two good ways, among many other options, to summarize a machine learning system for a group of customers, represented by an entire dataset, are variable importance charts and surrogate decision trees. Now, because I want business people to care about and understand my results, I’m going to call those two things a “main drivers chart” and a “decision flowchart,” respectively.The main drivers chart provides a visual summary and ranking of which factors are most important to a machine learning system’s decisions, in general. It’s a high-level summary and decent place to start communicating about how a machine learning system works.In the example problem, I’m trying to predict missed credit card payments in September, given payment statuses, payment amounts, and bill amounts from the previous six months. What Figure 2 tells me is that, for the machine learning system I’ve constructed, the previous month’s repayment status is by far the most important factor for most customers in my dataset. I can also see that July and June repayment statuses are the next most important factors.How did I make this chart? It’s just a slightly modified version of a traditional variable importance chart. To make sure the displayed information is as accurate as possible, I chose an interpretable model and matched it with a reliable variable importance calculation.When I know my results are mathematically solid, then I think about the presentation. In this case, first I removed all numbers from this chart. While numeric variable importance values might be meaningful to data scientists, most business people don’t have time to care about numbers that aren’t related to their business. I also replaced raw variable names with directly meaningful data labels, because no business person actually wants to think about my database schema.Once I’ve summarized my system in an understandable chart, I can go to my business partners and ask really important questions like: Is my system putting too much emphasis on August repayment status? Or, does it make sense to weigh April payment amount more than August payment amount? In my experience, accounting for those kinds of domain knowledge insights in my machine learning systems leads to the best technical and business outcomes.The decision flowchart shows how predictive factors work together to drive decisions in my machine learning system and Figure 3 boils my entire machine learning system down to one flowchart!How did I summarize an entire machine learning system into a flowchart? I used an old data mining trick known as a surrogate model.Unfortunately, this trick is not guaranteed to work every time. Sometimes machine learning systems are just too sophisticated to be accurately represented by a simple model. So a key consideration for data scientists when creating a chart like Figure 3 is: How accurate and stable is my decision tree surrogate model? On the business side, if a data scientist shows you a chart like Figure 3, you should challenge them to prove that it is an accurate and stable representation of the machine learning system.If you want to make a decision flow chart, remember to try to limit the complexity of the underlying machine learning system, keep your flow chart to a depth of say three to five decisions (Figure 3 uses a depth of three), and use human-readable data formats and not your favorite label encoder.If you work in financial services, you probably get that sometimes each individual machine learning system decision for each customer has to be explained or summarized. For the rest of the data science world, explaining individual machine learning system decisions might not be a regulatory requirement, but I would argue it’s a best practice. And regulations are likely on the way. Why not get ready?No one wants to be told “computer says no,” especially when the computer is wrong. So, consumer-level explanations are important for data scientists, who might want to override or debug bad machine learning behavior, and for consumers, who deserve to be able to appeal wrong decisions that affect them negatively.I’m going to focus on two types of explanations that summarize machine learning systems decisions for specific people, Shapley values (like in Figure 2) and counterfactual explanations. Since data science jargon isn’t helpful in this context, I’m going to call these two approaches main decision drivers (again) and “counter-examples.” Also, keep in mind there are many other options for creating consumer-specific explanations.   Shapley values can be used to summarize a machine learning system for an entire dataset (Figure 2) or at the individual decision level (Figure 4). When you use the right underlying machine learning and Shapley algorithm, these individual summaries can be highly accurate.Where I think most data scientists go wrong with Shapley values is in explaining them to business partners. My advice is don’t ever use equations, and probably don’t use charts or tables either. Just write out the elegant Shapley value interpretation in plain English (or whatever language you prefer). To see this approach in action, check out Figure 4. It displays the three most important drivers for a decision about a customer that my machine learning system has decided is at an above average risk for missing their September payment.Counter-examples explain what a customer could do differently to receive a different outcome from a machine learning system. You can sometimes use software librariesOnce you can see the logic and data points behind how a machine learning system makes a given decision, it becomes much easier for data scientists to catch and fix bad data or wrong decisions. It also becomes much easier for customers interacting with machine learning systems to catch and appeal the same kinds of wrong data or decisions.These kinds of explanations are also potentially useful for compliance with regulations like the Equal Credit Opportunity Act (ECOA) and the Fair Credit Reporting Act (FCRA) in the United States, and the General Data Protection Regulation (GDPR) in the European Union. The myriad risks of depending on a system you don’t understand are major barriers to the adoption of AI and machine learning in the business world. When you can break down those barriers, that’s a big step forward. Hopefully you’ll find the techniques I present here useful for just that, but do be careful. Aside from the accuracy and communication concerns I’ve already brought up, there are some security and privacy concerns with explainable ML too.Also, explainability is just one part of mitigating machine learning risks.All of this leads me into the responsible practice of machine learning, but that’s food for thought for my next article. Suffice to say, communicating machine learning system outcomes is incumbent on all data scientists in today’s data-driven world, and explaining AI and machine learning to a business decision maker is becoming more of a possibility with the right approach and right technology. Patrick Hall is a senior director for data science products at H2o.ai where he focuses mainly on model interpretability. Patrick is also currently an adjunct professor in the Department of Decision Sciences at George Washington University, where he teaches graduate classes in data mining and machine learning. Prior to joining H2o.ai, Patrick held global customer facing roles and R&D roles at SAS Institute. —1. Other great options for interpretable models include 2. I recommend monotonic gradient boosting machines plus 3. Decision tree surrogates go back to at least 4. Governments of at least 5. Like 6. Risks of 7. For a more thorough technical discussion of responsible machine learning see: “",https://www.infoworld.com/article/3533369/explaining-machine-learning-models-to-the-business.html
AI companies plant the seeds for quantum machine learning,"Quantum computing promises to accelerate analytics faster than the speed of light, but it still feels slightly unreal, in spite of the first signs of broad commercialization","Quantum isn’t the next big thing in advanced computing so much as a futuristic approach that could potentially be the biggest thing of all.Considering the theoretical possibility of quantum fabrics that enable seemingly magical, astronomically parallel, unbreakably encrypted, and faster-than-light subatomic computations, this could be the omega architecture in the evolution of AI (artificial intelligence).No one doubts that the IT industry is making impressive progress in developing and commercializing quantum technologies. But this mania is also shaping up to be the hype that ends all hype. It will take time for quantum technology to prove itself a worthy successor to computing’s traditional Though the splashy headlines boast of AI has been singled out as a quantum killer app for quite some time, but quantum so far has minimal presence in the commercial data analytics arena.We need to ask ourselves whether all the recent industry activity is setting ourselves up for the dreaded “quantum winter,” analogous to the long Nevertheless, there is a growing sense among researchers and even among analytics professionals that ML could become the core use case for quantum in our lives.This is not a recent revelation. More significantly, recent product launches and other announcements by In November 2019, Microsoft announced A month later, AWS announced the Then in January, Less than two weeks ago, The most important announcement, coming just a few days ago, was Google’s launch of Developed by Google’s X R&D unit, TensorFlow Quantum enables data scientists to use Python code to develop quantum ML models through standard Keras functions. It provides a library of quantum circuit simulators and quantum computing primitives that are compatible with existing TensorFlow APIs.TensorFlow Quantum’s release is no big surprise, coming several months after In addition to providing a full AI/ML software stack into which quantum processing can now be hybridized, Google intends to expand the range of more traditional chip architectures on which TensorFlow Quantum can simulate quantum ML. It has announced plans to expand the range of custom, quantum-simulation hardware platforms supported by the tool to include graphics processing units from various vendors as well as its own Recognizing that quantum computing is not yet mature enough to process the full range of ML workloads with sufficient accuracy, Google has wisely designed its new open source tool to support the many AI use cases with one foot in traditional computing architectures. TensorFlow Quantum enables developers to rapidly prototype ML models that hybridize the execution of quantum and classic processors in parallel on learning tasks. Using the tool, developers can build both classical and quantum datasets, with the classical data natively processed by TensorFlow, and the quantum extensions processing quantum data, which consists of both quantum circuits and quantum operators.Developers can use TensorFlow Quantum for supervised learning on such ML use cases as quantum classification, quantum control, and quantum approximate optimization. They can also execute advanced quantum learning tasks such as meta-learning, Hamiltonian learning, and sampling thermal states.In addition, Google has designed TensorFlow Quantum to support the growing range of AI use cases, such as “deepfakes” that do video, voice, and image generation with a high degree of verisimilitude. Google ML developers can use TensorFlow Quantum to train hybrid quantum/classical models to handle both the discriminative and generative workloads at the heart of the generative adversarial networks used in such applications.Strategically, Google’s likely next move will be to combine TensorFlow Quantum with its pre-existing Building “write once run anywhere” quantum ML will be tricky for quite some time, even in TensorFlow Quantum. This is because quantum researchers are experimenting with a wide range of alternative architectures even while they try to build practical applications.Here are some of the principal ways the leading quantum ML vendors are supporting these more fundamental quantum R&D requirements:Quantum computing has been in wait-and-see mode for so long that we tend to overlook the fact that it’s being rapidly put to practical uses.Even as quantum computing platform vendors experiment with new materials, methodologies, and architectures, researchers around the world have been demonstrating that quantum processing of ML models is in fact feasible. We can expect that AI/ML researchers at Google and elsewhere will probably use TensorFlow Quantum to do some fairly amazing things that were never feasible on traditional AI-accelerator hardware platforms.It’s clear from all these recent industry announcements that we’ll not only see commercialized quantum ML in our lifetime but that it has already begun to emerge and will gain steady adoption in this decade.",https://www.infoworld.com/article/3532436/ai-companies-plant-the-seeds-for-quantum-machine-learning.html
10 questions about deep learning ,"Learn why neural networks are so powerful, how and where they’re used, and how to get started — no programming necessary","It seems everywhere you look nowadays, you will find an article that describes a winning strategy using deep learning in a data science problem, or more specifically in the field of artificial intelligence (AI). However, clear explanations of deep learning, why it’s so powerful, and the various forms deep learning takes in practice, are not so easy to come by.In order to know more about deep learning, neural networks, the major innovations, the most widely used paradigms, where deep learning works and doesn’t, and even a little of the history, we have asked and answered a few basic questions.Deep learning is the modern evolution of traditional neural networks. Indeed, to the classic feed-forward, fully connected, backpropagation trained, multilayer perceptrons (MLPs), “deeper” architectures have been added. Deeper means more hidden layers and a few new additional neural paradigms, as in recurrent networks and in convolutional networks.There is no difference. Deep learning networks are neural networks, just with more complex architectures than were possible to train in the 1990s. For example, long short-term memory (LSTM) units in recurrent neural networks (RNNs) were introduced in 1997 by In general, faster and more powerful computational resources have allowed for the implementation and experimentation of more powerful and more promising neural architectures. It is clear that spending days in network training cannot rival the few minutes spent in training the same network with the help of GPU acceleration.The big breakthrough was in 2012 when the deep learning-based That’s particularly impressive if you consider that the human error rate is around 5 percent.In a word, flexibility. On the one hand, neural networks are universal function approximators, which is smart talk for saying that you can approximate almost anything using a neural network—if you make it complex enough. On the other hand, you can use the trained weights of a network to initialize the weights of another network that performs a similar task. This is called There are four very successful and widely adopted deep learning paradigms: LSTM units in recurrent neural networks, convolutional layers in convolutional neural networks (CNNs), encoder-decoder structures, and generative adversarial networks (GANs).RNNs are a family of neural networks used for processing sequential data, like text (e.g., a sequence of words or characters) or time series data. The idea is to apply a copy of the same network at each time step and connect the different copies via some state vectors. This allows the network to remember information from the past. Popular unit network structures in RNNs are gated recurrent units (GRUs) and LSTMs.CNN layers are especially powerful for data with spatial dependencies like images. Instead of connecting every neuron to the new layer, a sliding window is used, which works like a filter.  Some convolutions may detect edges or corners, while others may detect cats, dogs or street signs inside an image.Another often used neural network structure is the encoder-decoder network. A simple example is an autoencoder where a neural network with a bottleneck layer is trained to reconstruct the input to the output. A second application of encoder-decoder networks is neural machine translation where encoder-decoder structure is used in an RNN. The LSTM-based encoder extracts a dense representation of the content in the source language, and the LSTM-based decoder generates the output sequence in the target language.And, of course, the generative adversarial networks. A generative adversarial network is composed of two deep learning networks, the generator and the discriminator. Both networks are trained in alternating steps competing to improve themselves. GANs have been successfully applied to image tensors to create anime, human figures and even van Gogh-like masterpieces.No, at least not yet. There are certain domains, like computer vision, where you can’t get around deep learning anymore, but there are other areas, such as tabular data, that have proven to be a challenge for deep learning.In the case of tabular data, which is still the main format used for storing business data, deep learning is not doing terribly poorly there. However, training a deep learning model for days on an expensive GPU server is hard to justify if you can get similar accuracy using random forests or gradient boosted trees, which you can train within a few minutes on a decent laptop.Not really. It is true that most deep learning paradigms are available in TensorFlow and Keras and that both of them require Python skills. However, in our open source An example is shown in Figure 1 below, where we trained an LSTM-based RNN to generate free text. The model creates fake names that resemble mountain names for a new outdoor clothing line. At the top (the brown nodes), you can see where we built the neural architecture, which we then trained using the Keras Network Learner node. The trained network, opportunely modified, is then saved in a TensorFlow format.You can find plenty on our community KNIME Analytics Platform is an open source application and so are its integrations, including the Keras and TensorFlow integrations. You can install them wherever you wish, i.e. in the public cloud of your choosing or on your machine. It is clear, though, that the more powerful the machine, the faster the execution. You can even apply GPU acceleration in the KNIME Keras integration. You just need a GPU-equipped machine with Rosaria Silipo is principal data scientist at Kathrin Melcher is a data scientist at KNIME. She holds a master degree in mathematics from the University of Konstanz, Germany. She enjoys teaching and applying her knowledge to data science, machine learning and algorithms. Follow Kathrin on Adrian Nembach is a KNIME software engineer, specializing in machine learning algorithms including deep learning since 2015. He has an MSc in computer and information science from the University of Konstanz, where he focused on deep learning for computer vision. Follow Adrian on Corey Weisinger is a data scientist at KNIME in Austin, Texas. He studied mathematics at Michigan State University, focusing on actuarial techniques and functional analysis. Prior to KNIME, he worked as an analytics consultant for the auto industry in Detroit, Michigan. He currently focuses on signal processing and numeric prediction techniques and is the author of the guidebook, “For more information on KNIME, please visit —New Tech Forum provides a venue to explore and discuss emerging enterprise technology in unprecedented depth and breadth. The selection is subjective, based on our pick of the technologies we believe to be important and of greatest interest to InfoWorld readers. InfoWorld does not accept marketing collateral for publication and reserves the right to edit all contributed content. Send all inquiries to ",https://www.infoworld.com/article/3532058/10-questions-about-deep-learning.html
Not all AIops tools are created equal,"These tools can solve multicloud complexity issues, but they use very different approaches","AIops is becoming the new norm for operational tools supporting cloud operations. This technology can be applied to all types of operational tasks, providing intelligent automation that learns as it solves operational issues.These tools must carry out preprogrammed self-corrective processes, and the AIops tools’ ability to learn during those processes creates a huge advantage. For instance, understanding that performance issues could be saturation caused by cyberattacks should kick off security processes to mount a defense. Or moving out of a performance threshold should automatically launch more resources to bring performance back to an acceptable range.The number of things you can do with these tools increases each day, and it’s likely to be standard equipment for those of you deploying and operating multicloud.Most important, AIops tools can deal with thousands of data points and make correlations that most humans would not make. Moreover, as they correlate these data points the tool itself is smarter—it knows what the information actually means and how to assist the cloudops team.The trouble is that many products in this space are actually old technology made new. We’ve been using operational tools for years. Those tools were redone to support public clouds; now they have been rebranded as AIops tools with some built-in AI capabilities.The trouble with this type of evolution is that it’s happening so fast, the tools are naturally going to take different approaches. Some are very data driven, capable of analyzing historical data; other focus on real-time monitoring.Data-oriented tools look for patterns in the data—typically assisted by an AI engine—in order to find cause and effect. They get to the root cause of an issue without the cloudops staff having to cull through gobs of data.Also, AI is leveraged in different ways. Some have pretrained AI systems within the tools, which means the tools comes with a predetermined amount of knowledge. Others focus on training from scratch. Each approach has advantages.The bottom line is that AIops is still an emerging space, despite many vendors being more traditional technology players. Solution patterns differ from tool to tool, and thus you need to be more diligent to understand the basic functionality of the tools, as well as how to match your ops requirements with the proper offering.",https://www.infoworld.com/article/3537414/not-all-aiops-tools-are-created-equal.html
O’Reilly pulls the plug on in-person events,"The company believes the events business is ‘forever changed’ due to the coronavirus pandemic, so is moving its conferences online","In the wake of the COVID-19 virus pandemic, prominent technology conference producer O’Reilly has shut down its events business, permanently. From now on, O’Reilly events will be held online.The producer of events such as “Without understanding when this global health emergency may come to an end, we can’t plan for or execute on a business that will be forever changed as a result of this crisis,” said Laurie Baldwin, O’Reilly president. “With large technology vendors moving their events completely on-line, we believe the stage is set for a new normal moving forward when it comes to in-person events.”Baldwin noted that large technology vendors have moved events online as well. Microsoft, for one, is moving its Microsoft Build 2020 developer conference, originally planned for Seattle in May, to be all-digital.O’Reilly employees who had been involved in the in-person events business have been let go. In addition to the events business, O’Reilly has a technology publishing business and provides interactive coding events and custom training.",https://www.infoworld.com/article/3534429/oreilly-pulls-the-plug-on-in-person-events.html
How AI helped Domino’s improve pizza delivery,Sharing a container environment and Nvidia GPU server has enabled Domino’s data scientists to create more complex and accurate models to improve store and delivery operations,"When the words artificial intelligence (AI) and machine learning (ML) are used, people often think of advanced industries such as space exploration and biomedicine that rely heavily on research and development. The fact is, AI and ML should be something all industries are looking at, including retail. We are now in the customer service era and small differences in service can make a big difference in market share.This past week Nvidia held a Domino’s is an example of a familiar retail business that presented how it’s using AI and ML. The company has come up with a successful recipe to change the way it operates. The secret ingredient is Nvidia’s technology, which the leading pizza chain is using to improve store and online operations, provide a better customer experience, and route orders more efficiently.As a result, Domino’s is seeing happier customers and more tips for its drivers. But that’s only a small piece of the multifaceted pie. So, what does it take to get pizza from a Domino’s store to someone’s house? The answer is quite complex.The data science team at Domino’s tested the company’s speed and efficiency by leveraging Nvidia’s DGX-1 server, an integrated software and hardware system for deep learning research. For those not familiar with the DGX server line, Nvidia has created a series of turnkey appliances businesses can drop in and start using immediately. The alternative is to cobble together hardware, software, and AI platforms and tune the entire system correctly. This can take weeks to do.The Domino’s team created a delivery prediction model that forecasts when an order would be ready, using attributes of the order and what is happening in a Domino’s store, such as the number of employees, managers, and customers present at that moment. The model was based on a large dataset of five million orders, which isn’t massive but large enough to create accurate models. All future orders are fed back into the system to further increase model accuracy.Domino’s previous models used GPU-enabled laptops and desktops and would take more than 16 hours to train. The long timeframe made it extremely difficult to improve on the model, said Domino’s data science and AI manager Zachary Fragoso during a presentation at virtual GTC 2020.The extra compute power of the DGX-1 enabled Domino’s data scientists to train more complex models in less time. The system reduced the training time to under an hour and increased accuracy for order forecasts from 75 percent to 95 percent. The test demonstrated how Domino’s could boost productivity by training models faster, Fragoso said.Domino’s uncovered another benefit in the process: resource sharing. Each individual GPU on the DGX-1 is so large—with 32 GB of RAM—Domino’s data scientists could use a fraction of the GPUs and run multiple tests simultaneously. With eight such GPUs at their fingertips, the data scientists found themselves sharing resources and knowledge, as well as collaborating across teams.In the past, sharing work across teams—including code reviews and quality assurance testing—was challenging, since data scientists worked in their own local environments. Now that data scientists are working with a common DGX-1 server, they’re easily able to share Docker containers that are fully customizable and reproducible. This gives the data scientists a large resource pool to work with and access to resources when needed, so they’re not sitting idle. The Docker solution that Domino’s integrated with DGX-1 also makes it easier to reproduce code across different environments because all the data is contained within the Docker image.Domino’s recently purchased a second DGX-1 and started adding the Kubernetes container management system to the mix. With Kubernetes managed by an optimization engine, Domino’s can dynamically allocate resources to all its data scientists and launch containers faster. According to Fragoso, even data scientists who aren’t familiar with Linux can point-and-click to launch Docker containers.On the deployment side, Domino’s created an inferencing stack, which includes a Kubernetes cluster and four Nvidia GPUs. This way, data scientists can interact with and build their models using the same Docker container framework they use on the DGX-1.Domino’s also acquired a machine learning operations platform called Datatron, which sits on top of the Kubernetes cluster with the GPUs and assists Domino’s with ML-specific functionalities. Datatron allows for model performance monitoring in real time, so data scientists can be notified if their model requires retraining.Bringing the inference stack in-house allows Domino’s to have all the benefits that the cloud providers offer for hosting ML models, while keeping all data and resources on premises. It has changed the way the data scientists deploy models, giving them much more control over the deployment process, Fragoso explained in his presentation.Fragoso concluded with advice for other companies looking to bring these technologies in-house: “Think about how your data scientists will work together and collaborate. In our case, the DGX-1 and our data scientists are interacting in a common workspace. It was something that our team didn’t really consider when we first acquired this product and has been a real value for us.”Historically, data scientists operated as an independent silo within companies. More and more, IT organization are being asked to take on the task of providing the right technology to AI and ML initiatives. Data scientists are expensive resources for most companies and having them sit around waiting for models to finish is akin to tossing good pizza out the window. The right infrastructure, such as the DGX server series, enables companies to speed up processing time to let the data scientists work more and wait less.",https://www.infoworld.com/article/3535230/how-ai-helped-domino-s-improve-pizza-delivery.html
Is your data lake open enough? What to watch out for,"Like yesterday’s data warehouses, today’s data lakes threaten to lock us into proprietary formats and systems that restrict innovation and raise costs","A A challenge with data lakes is not getting locked into proprietary formats or systems. This lock-in restricts the ability to move data in and out for other uses or to process data using other tools, and can also tie a data lake to a single cloud environment. That’s why businesses should strive to build open data lakes, where data is stored in an open format and accessed through open, standards-based interfaces. Adherence to an open philosophy should permeate every aspect of the system, including data storage, data management, data processing, operations, data access, governance, and security. An open format is one based on an underlying open standard, developed and shared through a public, community-driven process without vendor-specific proprietary extensions. For example, an open data format is a platform-independent, machine-readable data format, such as ORC or Parquet, whose specification is published to the community, such that any organization can create tools and applications to read data in the format.A typical data lake has the following capabilities:In the following sections, we will describe openness requirements for each capability.An open data lake ingests data from sources such as applications, databases, data warehouses, and real-time streams. It formats and stores the data into an open data format, such as ORC and Parquet, that is platform-independent, machine-readable, optimized for fast access and analytics, and made available to consumers without restrictions that would impede the re-use of that information. An open data lake supports both pull-based and push-based ingestion of data. It supports pull-based ingestion through batch data pipelines and push-based ingestion through stream processing. For both these types of data ingestion, an open data lake supports open standards such as The ingest capability of an open data lake ensures zero data loss and writes exactly-once or at-least-once, handles schema variability, writes in the most optimized data format into the right partitions, and provides the ability to re-ingest data when needed.An open data lake stores the raw data from various data sources in a standardized open format. However, use cases such as data exploration, interactive analytics, and machine learning require that the raw data be processed to create use-case driven trusted data sets. For data exploration and machine learning use cases, users continually refine data sets for their analysis needs. As a result, every data lake implementation should enable users to iterate between data engineering and use cases such as interactive analytics and machine learning. This can be thought of as continuous data engineering, which involves the interactive ability to author, monitor, and debug data pipelines. In an open data lake, these pipelines are authored using standard interfaces and open source tools such as SQL, Python, Apache Spark, and Apache Hive.The most visible outcome of the data lake is the types of use cases it enables. Whether the use case is data exploration, interactive analytics, or machine learning, access to data is vital. The access to data can be through SQL or programmatic languages such as Python, R, and Scala. While SQL is the norm for interactive analysis, programmatic languages are used for more advanced applications like machine learning and deep learning. An open data lake supports data access through a standards-based implementation of  SQL with no proprietary extensions. It enables external tools to access that data through standards such as ODBC and JDBC. Also, an open data lake supports programmatic access to data via standard programming languages such as R, Python, and Scala, and standard libraries for numerical computation and machine learning, such as TensorFlow, Keras, PyTorch, Apache Spark MLlib, MXNet, and Scikit-learn.When data ingestion and data access are implemented well, data can be made widely available to users in a democratized fashion. When multiple teams start accessing data, data architects need to exercise oversight for governance, security, and compliance purposes. Data itself is hard to find and comprehend and not always trustworthy. Users need the ability to discover and profile data sets for integrity before they can trust them for their own use case. A data catalog enriches metadata through different mechanisms, uses it to document data sets, and supports a search interface to aid discovery.Since the first step is to discover the required data sets, it’s essential to surface metadata to end-users for exploration purposes, to see where the data resides and what it contains, and to determine if it is useful for answering a particular question. Discovery includes data profiling capabilities that support interactive previews of data sets to shine a light on formatting, standardization, labels, data shape, and so on.An open data lake should have an open metadata repository. As an example, the Apache Hive metadata repository is an open repository that prevents vendor lock-in for metadata.Increasing accessibility to the data requires data lakes to support strong access control and security features. To be open, a data lake should do this through non-proprietary security and access control APIs. As an example, deep integration with open source frameworks such as Apache Ranger and Apache Sentry can facilitate table-level, row-level, and column-level granular security. This enables administrators to grant permissions against already-defined user roles in enterprise directories such as Active Directory. By basing access control on open source frameworks, open data lakes avoid vendor lock-in that results from a proprietary security implementation.New or expanded data privacy regulations, such as GDPR and CCPA, have created new requirements around “Right to Erasure” and “Right to Be Forgotten.” These govern consumers’ rights about their data and involve stiff financial penalties for non-compliance (as much as four percent of global turnover), so they must not be overlooked. Therefore, the ability to delete specific subsets of data without disrupting a data management process is essential. An open data lake supports this ability through open formats and open metadata repositories. In this way, they enable a vendor-agnostic solution to compliance needs.Whether the data lake is deployed in the cloud or on-premises, each cloud provider has a specific implementation to provision, configure, monitor, and manage the data lake as well as the resources it needs. An open data lake is cloud-agnostic and portable across any cloud-native environment, including public and private clouds. This allows administrators to leverage the benefits of both public and private cloud from an economics, security, governance, and agility perspective.  The increase in the volume, velocity, and variety of data, combined with new types of analytics and machine learning, make data lakes a necessary complement to more traditional data warehouses. Data warehouses exist largely in a world of proprietary formats, proprietary SQL extensions, and proprietary metadata repositories, and lack programmatic access to data. Data lakes don’t need to follow this proprietary path, which leads to restricted innovation and higher costs. A well designed, open data lake provides a robust, future-proof data management system that supports a wide range of data processing needs including data exploration, interactive analytics, and machine learning.Ashish Thusoo is co-founder and chief executive officer of —New Tech Forum provides a venue to explore and discuss emerging enterprise technology in unprecedented depth and breadth. The selection is subjective, based on our pick of the technologies we believe to be important and of greatest interest to InfoWorld readers. InfoWorld does not accept marketing collateral for publication and reserves the right to edit all contributed content. Send all inquiries to ",https://www.infoworld.com/article/3534516/is-your-data-lake-open-enough-what-to-watch-out-for.html
InfoWorld’s 2020 Technology of the Year Award winners,"InfoWorld recognizes the year’s best products in software development, cloud computing, data analytics, and machine learning",,https://www.infoworld.com/article/3518995/infoworlds-2020-technology-of-the-year-award-winners.html
Boosting AI’s smarts in the absence of training data,Zero-shot learning repurposes knowledge through statistical or semantic approaches without needing huge amounts of fresh training data,"AI (artificial intelligence) is the most perfect field of dreams in modern culture. If you ask the average person on the street what AI runs on, they probably won’t mention training data. Instead, they might mumble something about computer programs that magically learn how to do useful stuff from thin air.However, some of today’s most sophisticated AI comes close to that naïve dream. I’m referring to a still-developing approach known as “zero-shot learning.” This methodology—which is being explored at Zero-shot pattern learning will enable intelligent robots to dynamically recognize and respond to unfamiliar objects, behaviors, and environmental patterns that they may never have encountered in training. I predict that zero-shot approaches will increasingly be combined with reinforcement learning in order to enable robots to take the best actions iteratively in environments that are chaotic and one-off.In addition, gaming applications will use zero-shot approaches such as Furthermore, zero-shot learning promises to make object recognition applications more versatile, due to its ability to drive:What makes zero-shot learning possible is the existence of prior knowledge that can be discovered and repurposed through statistical or semantic approaches. Zero-shot methods use this knowledge to predict the larger semantic space of features that encompasses both the seen instances (those in the training data) and the unseen instances (those missing from training data). Regarding automated knowledge discovery, some of the most promising technical approaches for zero-shot learning include:Zero-shot learning can’t realize its potential as an AI pipeline accelerator unless data scientists acquire tools that provides simplified access to these techniques. That, in turn, requires deep learning toolkits that support easy visual design of new models from pre-existing functional building blocks under a larger paradigm known as “As zero-shot techniques gain adoption, and the pool of prior knowledge grows, developers of high-quality AI will grow less reliant on training data. During the next few years, we’ll see data scientists build more intelligent robotics, gaming, and pattern-recognition applications by configuring pre-existing statistical and semantic knowledge, without needing to acquire, prepare, and label huge amounts of fresh training data.When that day arrives, more AI-based applications will be able to automate the bootstrapping of their intelligence from a state of pure ignorance to one of deep knowledge through techniques that are ad-hoc, zero-shot, and situationally adaptive.That will mark the true beginning of artificial general intelligence, a dream that has motivated the AI community from the days of Alan Turing all the way to the present.",https://www.infoworld.com/article/3529871/boosting-ais-smarts-in-the-absence-of-training-data.html
8 great Python libraries for natural language processing,"With so many NLP resources in Python, how to choose? Discover the best Python libraries for analyzing text and how to use them","Natural language processingThe Python language provides a convenient front-end to all varieties of Note that some of these libraries provide higher-level versions of the same functionality exposed by others, making that functionality easier to use at the cost of some precision or performance. You’ll want to choose a library well-suited both to your level of expertise and to the nature of the project.The CoreNLP includes a The easiest place to start with CoreNLP’s Python wrappers is CoreNLP also supports the use of NLTK, a major Python NLP library discussed below. As of version 3.2.3, NLTK includes interfaces to CoreNLP in its parser. Just be sure to The obvious downside of CoreNLP is that you’ll need some familiarity with Java to get it up and running, but that’s nothing a careful reading of the documentation can’t achieve. Another hurdle could be CoreNLP’s licensing. The whole toolkit is licensed under the GPLv3, meaning any use in proprietary software that you distribute to others will require GensimGensim can work with very large bodies of text by streaming documents to its analysis engine and performing Gensim’s detailed documentation includes tutorials and how-to guides that explain key concepts and illustrate them with hands-on examples. Common recipes are also available on the The As the documentation states, NLTK provides a wide variety of tools for working with text: “classification, tokenization, stemming, tagging, parsing, and semantic reasoning.” It can also work with Keep in mind that NLTK was created by and for an academic research audience. It was not designed to serve NLP models in a production environment. The documentation is also somewhat sparse; even the If you are determined to leverage what’s inside NLTK, you might start instead with TextBlob (discussed below).If all you need to do is scrape a popular website and analyze what you find, reach for Pattern comes with built-ins for scraping a number of popular web services and sources (Google, Wikipedia, Twitter, Facebook, generic RSS, etc.), all of which are available as Python modules (e.g., Pattern exposes some of its lower-level functionality, allowing you to to use NLP functions, n-gram search, vectors, and graphs directly if you like. It also has a built-in helper library for working with common databases (MySQL, SQLite, and MongoDB in the future), making it easy to work with tabular data stored from previous sessions or obtained from third parties.PolyglotThe NLP features in Polyglot echo what’s found in other NLP libraries: tokenization, named entity recognition, part-of-speech tagging, sentiment analysis, word embeddings, etc. For each of these operations, Polyglot provides models that work with the needed languages.Note that Polyglot’s language support differs greatly from feature to feature. For instance, the tokenization system supports almost 200 languages (largely because it uses the Unicode Text Segmentation algorithm), and sentiment analysis supports 136 languages, but part-of-speech tagging supports only 16.PyNLPIMost of the NLP functions in PyNLPI are for basic jobs like tokenization or n-gram extraction, along with some statistical functions useful in NLP like Levenshtein distance between strings or Markov chains. Those functions are implemented in pure Python for convenience, so they’re unlikely to have production-level performance.But PyNLPI shines for working with some of the more exotic data types and formats that have sprung up in the NLP space. PyNLPI can read and process GIZA, Moses++, SoNaR, Taggerdata, and TiMBL data formats, and devotes an entire module to working with You’ll want to reach for PyNLPI whenever you’re dealing with those data types.SpaCySpaCy includes most every feature found in those competing frameworks: speech tagging, dependency parsing, named entity recognition, tokenization, sentence segmentation, rule-based match operations, word vectors, and tons more. SpaCy also includes optimizations for GPU operations—both for accelerating computation, and for storing data on the GPU to avoid copying.Spacy’s documentation is excellent. A setup wizard generates command-line installation actions for Windows, Linux, and macOS and for different Python environments (pip, conda, etc.) as well. Language models install as Python packages, so they can be tracked as part of an application’s dependency list.TextBlobTextBlob smooths the way by leveraging native Python objects and syntax. The Another advantage of TextBlob is you can “lift the hood” and alter its functionality as you grow more confident. Many default components, like the sentiment analysis system or the tokenizer, can be ",https://www.infoworld.com/article/3519413/8-great-python-libraries-for-natural-language-processing.html
A brief history of artificial intelligence,"Despite huge advances in machine learning models, AI challenges remain much the same today as 60 years ago","In the early days of artificial intelligence, computer scientists attempted to recreate aspects of the human mind in the computer. This is the type of intelligence that is the stuff of science fiction—machines that think, more or less, like us. This type of intelligence is called, unsurprisingly, intelligibility. A computer with intelligibility can be used to explore how we reason, learn, judge, perceive, and execute mental actions.Early research on intelligibility focused on modeling parts of the real world and the mind (from the realm of cognitive scientists) in the computer. It is remarkable when you consider that these experiments took place nearly 60 years ago.Early models of intelligence focused on deductive reasoning to arrive at conclusions. One of the earliest and best known A.I. programs of this type was the Logic Theorist, written in 1956 to mimic the problem-solving skills of a human being. The Logic Theorist soon proved 38 of the first 52 theorems in chapter two of the Soon research turned toward a different type of thinking, inductive reasoning. Inductive reasoning is what a scientist uses when examining data and trying to come up with a hypothesis to explain it. To study inductive reasoning, researchers created a cognitive model based on the scientists working in a NASA laboratory, helping them to identify organic molecules using their knowledge of organic chemistry. The Dendral program was the first real example of the second feature of artificial intelligence, Dendral was unique because it also included the first knowledge base, a set of if/then rules that captured the knowledge of the scientists, to use alongside the cognitive model. This form of knowledge would later be called an By the late 1960’s the answer was clear. The performance of Dendral was almost completely a function of the amount and quality of knowledge obtained from the experts. The cognitive model was only weakly related to improvements in performance.This realization led to a major paradigm shift in the artificial intelligence community. Knowledge engineering emerged as a discipline to model specific domains of human expertise using expert systems. And the expert systems they created often exceeded the performance of any single human decision maker. This remarkable success sparked great enthusiasm for expert systems within the artificial intelligence community, the military, industry, investors, and the popular press.As expert systems became commercially successful, researchers turned their attention to techniques for modeling these systems and making them more flexible across problem domains. It was during this period that object-oriented design and hierarchical ontologies were developed by the AI community and adopted by other parts of the computer community. Today hierarchical ontologies are at the heart of knowledge graphs, which have seen a resurgence in recent years.As researchers settled on a form of knowledge representation known as “production rules,” a form of first order predicate logic, they discovered that the systems could learn automatically; i.e., the systems coud write or rewrite the rules themselves to improve performance based on additional data. Dendral was modified and given the ability to learn the rules of mass spectrometry based on the empirical data from experiments.As good as these expert systems were, they did have limitations. They were generally restricted to a particular problem domain, and could not distinguish from multiple plausible alternatives or utilize knowledge about structure or statistical correlation. To address some of these issues, researchers added certainty factors—numerical values that indicated how likely a particular fact is true.The start of the second paradigm shift in AI occurred when researchers realized that certainty factors could be wrapped into statistical models. Statistics and Bayesian inference could be used to model domain expertise from the empirical data. From this point forward, artificial intelligence would be increasingly dominated by machine learning.There is a problem, though. Although machine learning techniques such as random forest, neural networks, or GBTs (gradient boosted trees) produce accurate results, they are nearly impenetrable black boxes. Without intelligible output, machine learning models are less useful than traditional models in several respects. For example, with a traditional AI model, a practitioner might ask:The lack of intelligibility has training implications as well. When a model breaks, and cannot explain why, it makes it more difficult to fix. Add more examples? What kind of examples? Although there are some simple trade-offs we can make in the interim, such as accepting less accurate predictions in exchange for intelligibility, the ability to explain machine learning models has emerged as one of the next big milestones to be achieved in AI.They say that history repeats itself. Early AI research, like that of today, focused on modeling human reasoning and cognitive models. The three main issues facing early AI researchers—knowledge, explanation, and flexibility—also remain central to contemporary discussions of machine learning systems.Knowledge now takes the form of data, and the need for flexibility can be seen in the brittleness of neural networks, where slight perturbations of data produce dramatically different results. Explainability too has emerged as a top priority for AI researchers. It is somewhat ironic how, 60 years later, we have moved from trying to replicate human thinking to asking the machines how they think.",https://www.infoworld.com/article/3527209/a-brief-history-of-artificial-intelligence.html
Can you put your trust in AIops?,"As ops tools morph into AIops, they bring greater value to more complex deployments—if users will let them","AIops (artificial intelligence for IT operations) is one of those cool buzzwords that is actually part of another buzzword: cloudops (cloud operations), which is a part of the mother of all buzzwords: The concept of AIops and the tool category of AIops are really the maturation of operational tools in general. Most of those in the traditional ops tools space, at least in the past few years, bolted an AI engine onto a tool and called it AIops.Some purpose-built AIops tool startups out there are leveraging AI from the jump. All are worth a look as you select AIops tools; however, there are no mainstream brands. The objective was and is obvious. Since most of these tools have been data gathering tools and analytics tools from the beginning, adding AI allows them to learn from that data rather than just externalize issues with the services under management. In some cases, they can correct issues using preprogrammed routines, such as restarting a server or blocking an IP address that seems to be attacking one of your servers.Now that we’re a few years into this paradigm and its technology offerings, we’re starting to note some patterns—some good, and some not so good. Let’s explore both. As far as what’s working, AIops tools in many instances are ops tools in their fourth, fifth, or sixth generations. Moreover, most of them have had public cloud management in mind for a while and are able to bridge the gap between on-premises legacy system management and managing applications and services in the public clouds.They are capable tools for managing and monitoring cloud, multicloud, legacy, and even IoT and edge-based systems. This ability to support complex system heterogeneity is really the true value of the ops tools, and why they are important to those implementing cloud or noncloud systems. The downside seems to be that most users are not taking advantage of the AI subsystems in the tools, so you may be paying for a feature you’re not using. I don’t view this as the fault of the tool provider; for the most part, this relates to how the tools are installed, set up, and used by the traditional and cloudops teams. This is due to a lack of training in some cases, or a lack of valid use cases in the current set of systems—cloud and not cloud—under management. Clearly, AIops is going to be part of most cloud-based deployments. Just as clearly, the more complex those deployments, such as multicloud, the more value they will bring.",https://www.infoworld.com/article/3526444/the-state-of-aiops.html
RStudio Conference 2020 videos you don’t want to miss,You can watch dozens of must-see RStudio Conference videos online. Don't know where to start? Let us help,"With dozens of RStudio Conference videos now available online, it’s hard to know where to begin. I hope this look at some of my favorites will help get you started!I could probably watch Jenny Bryan teach data Wondering what features are coming to the next version of RStudio desktop? RStudio’s Jonathan McPherson outlined several, including modern-era spell check (at last), better cloud usability on iOS, and more screen-reader accessibility for visually impaired users—something that also improves keyboard navigation for all users. Video: RStudio Chief Scientist Hadley Wickham reviewed last year’s highlights from the tidyverse and this year’s plans for further development, but he was also fairly forthright in discussing some recent missteps.In particular, he acknowledged that the initial rollout of “tidy evaluation” launched with a somewhat difficult-to-master syntax and an unreasonable expectation that users would want to learn the detailed computing theory behind it. It turned out that many users didn’t care about the mechanics behind incorporating the tidyverse into their own custom functions; they just wanted to write their code. Since then, tidy eval syntax has been changed to more understandable Wickham also outlined how tidyverse package authors will help users better understand the lifecycle of older functions and if/how some functions may be deprecated. Video: Claus Wilke gave an overview of the ggtext package in this fast-paced presentation, showing how to customize ggplot visualizations with colored text, images on axes, and more. He also explained the package’s current limits. Video: Below, you can also watch my I’ve used scales package functions such as Shiny creator and RStudio CTO Joe Cheng demo’d What do you get when you have a load of items plotted over time? That data type is known as longitudinal, and visualizing it can often end up looking like a pile of spaghetti. To help solve this problem, Nicholas Tierney at Monash University created the This wasn’t R-specific, but University of Pennsylvania dataviz specialist Will Chase gave an engaging, opinionated talk on how to “take your charts from drab to fab.” One tip: “White space is like garlic — take as much as you think you need and triple it.” Video: There is a lot more one can do with R Markdown than I thought. And the fun-as-well-as-educational Teacup Giraffe site pushes the limits. In addition to enjoying a look at the And speaking of getting more out of markdown, RStudio’s Yihui Xie had a separate talk showing how to generate many more file types than just HTML or PDF from an R Markdown document. Video: I’d been resisting razzle-dazzle around the Morgan-Wall showed how to turn a conventional graphic into a 3D visualization and animation with very little code. He also showed some recent package improvements that make some graphics more visually striking. In this case, seeing the animated examples is a lot better than trying to read about them. If you’re at all interested in this package, it’s worth watching the presentation. Video: The Apache Arrow project is a multi-language standard for in-memory data aimed at interoperability and high performance. Arrow has been implemented in R with the If you’ve seen the heated discussions on social media, you might think that tidyverse and data.table are in two opposing camps. But while each has its fans, there are an increasing number of people who use both. Utah State Research Assistant Professor Tyson S. Barrett is one, and he brought data.table to RStudio Conference with a talk on using complex list-columns with data.table Barrett also mentioned his Not familiar with data.table? Check out my Do More With R 5-minute intro below. There were a lot of interesting lightning talks, but a few stood out in part for the cool websites and packages being demo’d as well as the presentations themselves.RStudio intern Maya Gans showed a drag-and-drop interface for tidyverse tasks such as transforming, summarizing, and plotting data. It’s an interesting way to teach tidyverse concepts before students have to learn actual code. Video: The still-experimental Data science for software engineers: Busting software myths with RRStudio founder and CEO J.J. Allaire discussed the state of open source software, how it’s possible to fund open source efforts, and the company’s move to Every time I see Martin Wattenberg and Fernanda Viegas speak, I leave feeling grateful that I’ve got a job that lets me peek into the work and thoughts of some supersmart people. Co-leaders of A final note: With multiple tracks going on at once, I missed a lot of excellent talks. I also attended other good ones that didn’t make the list because I didn’t want this article to get too long. For example, Want more R tips? Check out ",https://www.infoworld.com/article/3528303/rstudio-conference-2020-videos-you-dont-want-to-miss.html
5 reasons to choose PyTorch for deep learning,"TensorFlow still has certain advantages, but a stronger case can be made for PyTorch every day","PyTorchBefore we get started, a plea to One of the primary reasons that people choose PyTorch is that the code they look at is fairly simple to understand; the framework is designed and assembled to work with Python instead of often pushing up against it. Your models and layers are simply Python classes, and so is everything else: optimizers, data loaders, loss functions, transformations, and so on.Due to the eager execution mode that PyTorch operates under, rather than the static execution graph of traditional TensorFlow (yes, TensorFlow 2.0 does offer eager execution, but it’s a touch clunky at times) it’s very easy to reason about your custom PyTorch classes, and you can dig into debugging with PyTorch also has the plus of a stable API that has only had one major change from the early releases to version 1.3 (that being the change of Variables to Tensors). While this is undoubtedly due to its young age, it does mean that the vast majority of PyTorch code you’ll see in the wild is recognizable and understandable no matter what version it was written for.While the “batteries included” philosophy is definitely not exclusive to PyTorch, it’s remarkably easy to get up and running with PyTorch. Using And PyTorch Hub is unified across domains, making it a one-stop shop for architectures for working with text and audio as well as vision.As well as models, PyTorch comes with a long list of, yes, loss functions and optimizers, like you’d expect, but also easy-to-use ways of loading in data and chaining built-in transformations. It’s also rather straightforward to build your own loaders or transforms. Because everything is Python, it’s simply a matter of implementing a standard class interface.One little note of caution is that a PyTorch is heaven for researchers, and you can see this in its use in papers at all major deep learning conferences. In 2018, PyTorch was growing fast, but in 2019, it has become the framework of choice at Experimenting with new concepts is much easier when creating new custom components is a simple, stable subclass of a standard Python class. And the flexibility offered means that if you want to write a layer that sends parameter information to TensorBoard, ElasticSearch, or an Amazon S3 bucket... you can just do it. Want to pull in esoteric libraries and use them inline with network training or an odd new attempt at a training loop? PyTorch is not going to stand in your way.One thing holding PyTorch back a little has been the lack of a clear path from research to production. Indeed, TensorFlow still rules the roost for production usage, no matter how much PyTorch has taken over research. But with PyTorch 1.3 and the expansion of There are dozens of deep learning courses out there, but for my money the In the most recent version of the course, you’ll discover how to achieve state-of-the-art results on tasks such as classification, segmentation, and predictions in text and vision domains, along with learning all about While the fast.ai course uses fast.ai’s own library that provides further abstractions on top of PyTorch (making it even easier to get to grips with deep learning), the course also delves deep into the fundamentals, building a PyTorch-like library from scratch, which will give you a thorough understanding of how the internals of PyTorch actually work. The fast.ai team even manages to fix some bugs in mainline PyTorch along the way. Finally, the PyTorch community is a wonderful thing. The main website at Beyond the official documentation, the Discourse-based forum at discuss.pytorch.org is an amazing resource where you can easily find yourself talking to and being helped out by core PyTorch developers. With over fifteen hundred posts a week, it’s a friendly and active community. And while discussion is more focused on fast.ai’s own library, the similar forums over at forums.fast.ai is another great community (with lots of crossover) that is eager to help newcomers in a non-gatekeeping manner, which sadly is a problem in many arenas of deep learning discussion.There you have it—five reasons to use PyTorch. As I said at the beginning, not all of these are exclusive to PyTorch versus competitors, but the combination of all of these reasons makes PyTorch my deep learning framework of choice. There are definitely areas where PyTorch is currently deficient — e.g., in mobile, with sparse networks, and easy quantizing of models, just to pick three out of the hat. But given the high speed of development, PyTorch will be a much stronger performer in these areas by year’s end.A couple of further examples just to finish us out. First, Second, Coming in the wake of ",https://www.infoworld.com/article/3528780/5-reasons-to-choose-pytorch-for-deep-learning.html
Swift language targets machine learning,Swift 6 roadmap also emphasizes expressive and elegant APIs and ‘fantastic’ development experience,"Moving toward Swift 6, the core development team behind Ambitions in the machine learning space are part of plans to invest in “user-empowering directions” for the language. Apple is not the only company with machine learning ambitions for Swift; Google has integrated Swift with the TensorFlow machine learning library in a project called In addition to machine learning, directions eyed for Swift include building APIs such as Introduced in ",https://www.infoworld.com/article/3526594/swift-language-targets-machine-learning.html
Kubeflow 1.0 solves machine learning workflows with Kubernetes,Google's machine learning toolkit for Kubernetes helps data scientists manage machine learning workflows and deploy and scale models in production ,"KubeflowKubeflow was built to address two major issues with machine learning projects: the need for Kubeflow is designed to manage every phase of a machine learning project: writing the code, building the containers, allocating the Kubernetes resources to run them, training the models, and serving predictions from those models. The Kubeflow 1.0 release provides tools, such as Jupyter notebooks for working with data experiments and a web-based dashboard UI for general oversight, to help with each phase.Google claims Kubeflow provides repeatability, isolation, scale, and resilience not just for model training and prediction serving, but also for development and research work. Jupyter notebooks running under Kubeflow can be resource-limited and process-limited, and can re-use configurations, access to secrets, and data sources.Several Kubeflow components are still under development and will be rolled out in the near future. ",https://www.infoworld.com/article/3529977/kubeflow-10-solves-machine-learning-workflows-with-kubernetes.html
InfoWorld Technology of the Year Awards,"About InfoWorld’s annual awards for the best hardware, software, and cloud services of the year","InfoWorld’s annual Technology of the Year Awards recognize the best and most innovative products in the areas of software development, cloud computing, big data analytics, and machine learning.Note that we do not have a formal submission process for the Technology of the Year Awards. Winners are chosen based on our coverage of products throughout the year. However, we welcome suggestions regarding products we should consider for coverage and for awards at any time. To have your hardware, software, or cloud service considered for a 2019 Technology of the Year Award, please send the name of the product and a link to product information to Executive Editor The 2020 Technology of the Year Award winners will be announced February 3, 2021.Marketing/PR",https://www.infoworld.com/article/2871319/infoworld-technology-of-the-year.html
Microsoft speeds up PyTorch with DeepSpeed,A new open source project from Microsoft accelerates PyTorch machine learning without major code rewrites,"Microsoft has released According to It’s the minimal impact on existing PyTorch code that has the greatest potential impact. As machine learning libraries grow entrenched, and more applications become dependent on them, there is less room for new frameworks, and more incentive to make existing frameworks more performant and scalable.PyTorch is already fast when it comes to both computational and development speed, but there’s always room for improvement. Applications written for PyTorch can make use of DeepSpeed with only minimal changes to the code; there’s no need to start from scratch with another framework.One way DeepSpeed enhances PyTorch is by improving its native parallelism. In one example, provided by Microsoft in the DeepSpeed documentation, attempting to train a model using PyTorch’s Distributed Data Parallel system across Nvidia V100 GPUs with 32GB of device memory “[ran] out of memory with 1.5 billion parameter models,” while DeepSpeed was able to reach 6 billion parameters on the same hardware.Another touted DeepSpeed improvement is more efficient use of GPU memory for training. By partitioning the model training across GPUs, DeepSpeed allows the needed data to be kept close at hand, reduces the memory requirements of each GPU, and reduces the communication overhead between GPUs.A third benefit is allowing for more parameters during model training to improve prediction accuracy. To eliminate the need for expertise and human effort, many machine learning frameworks now support some kind of automated hyperparameter optimization. With DeepSpeed, Microsoft claims that “deep learning models with 100 billion parameters” can be trained on “the current generation of GPU clusters at three to five times the throughput of the current best system.”DeepSpeed is available as free open source under the MIT License. Tutorials in the official repo ",https://www.infoworld.com/article/3526449/microsoft-speeds-up-pytorch-with-deepspeed.html
InfoWorld Technology of the Year Awards promotional information,Guidelines for InfoWorld Technology of the Year Award winners,"Congratulations! To help you promote your win, InfoWorld has supplied the following PR information and usage guidelines.Note that companies named as an If you would like a physical award, or print or electronic reprints, you can place an order through the YGS Group via phone at (800) 290-5460 x129 or via email at As an InfoWorld Technology of the Year winner, you have the opportunity to purchase a license for the rights to use the Technology of the Year logo. Please contact the YGS Group via phone at (800) 290-5460 x129 or via email at The InfoWorld Technology of the Year brand is a valuable asset that International Data Group needs to protect. We ask that you help us by properly using the logo in accordance with our guidelines listed below. Accordingly, we ask that your business partners, customers, and other third parties adhere to the guidelines listed below.Parties given permission to use the InfoWorld Technology of the Year logotype must adhere to the following rules:If you are unsure if your usage is within these guidelines, please Please read the following guidelines carefully before preparing a press release that references InfoWorld and the InfoWorld Technology of the Year rankings.If your company wants to include a quote attributed to an InfoWorld spokesperson, please use the following quote to reinforce InfoWorld Technology of the Year key messages. Modified versions of this quote are subject to approval from InfoWorld. All quotes should be attributed to Doug Dineley, Executive Editor, InfoWorld.“If digital transformation means anything, it means taking advantage of the latest advances in software development, cloud computing, data analytics, and AI to improve your business,” said Doug Dineley, executive editor of InfoWorld. “Our 2020 Technology of the Year Award winners are the platforms and tools that the most innovative companies are using to tap the power of data, streamline business processes, and respond more quickly to customers and new business opportunities.” All communications involving InfoWorld and the InfoWorld Technology of the Year Awards must be consistent with the style and content listed below:The following approved InfoWorld corporate boilerplate and short InfoWorld Technology of the Year Awards description may be used, where appropriate, in press releases referencing inclusion in the InfoWorld Technology of the Year feature.Selected by InfoWorld editors and reviewers, the annual awards identify the best and most innovative products on the IT landscape. Winners are drawn from products tested during the past year, with the final selections made by InfoWorld’s Reviews staff. About InfoWorldAbout IDG Communications  ",https://www.infoworld.com/article/2871225/infoworld-technology-of-the-year-awards-promotional-information.html
Use Azure Cognitive Services to automate forms processing,"Form Recognizer brings unsupervised machine learning to paper document processing, and it’s a snap to  build into your applications","Microsoft’s Cognitive Services, That’s an important difference between machine learning and other, more familiar, algorithms. As Microsoft improves its training and models, the scope of the services continues to get better, along with responsiveness and accuracy. Some can even take advantage of a process called Transfer Learning, where training a model with one set of data improves its performance with another.Continuous improvement isn’t the only benefit of the research work Microsoft puts into its Microsoft has been able to One of the more interesting new services currently in preview is Form Recognizer takes a more nuanced approach to working with form data, As Form Recognizer is an API, you can incorporate it in new and existing business processes, replacing manual data capture processes while flagging exceptions that may need human intervention. You can even use Form Recognizer in conjunction with Power Platform tools such as Power BI to deliver business insights from what would have been paper-only data.One of the interesting aspects of Form Recognizer is that the underlying model uses unsupervised learning. There’s no need to label the training data. The system recognizes the form elements and generates the appropriate data structures for your form data. Although that’s an easier way to train a system, you do have the option of using labeled data to get more accurate and faster results.A key element of the training process is the layout API. This gives the model a structure for the layout of a form, with labels for the various fields. Using the data from this and from labeled training forms, you can quickly define the output data structures and ensure that your code is ready to work with the service.Building labeled samples for trainingOnce trained, you have a custom Form Recognizer model with its own model ID and an accuracy score. If you want to improve the model, add more sample data. The resulting model can be tested using the training tool on documents that haven’t been part of your training set. You’ll be presented with a view of the source document with bounding boxes for recognized data and a confidence level for each element. It’s important to note that Form Recognizer can’t work with all form elements; at the moment there’s no support for check boxes or for complex tables.Building an application around Form Recognizer is relatively easy. If you’re not using a language with a supported SDK there’s a REST API that The API is relatively simple; it uses POST to upload and analyze the form contents, with a GET to bring back the result. Images are sent as part of a JSON object with the POST, or as a standard file stream. Once the job has been loaded, a standard HTTP 2020 response returns the result ID that will hold the analysis results. You can then make a call to the service with the result ID. If the form has been processed, the results will be delivered in a JSON object that can be parsed, delivering the form key/value pairs and any result tables.Like all the Cognitive Services, the results have a confidence level. You can use this to direct some forms for manual checks, otherwise delivering the result data into your line of business applications, either storing the data for future use or using it to drive a business process.One useful feature of Form Recognizer (and one that clearly builds on Microsoft’s own requirements for its expense system) is a prebuilt model that works with common U.S. receipt formats. You can use it to capture and feed receipt data into your own expenses workflow, using a phone camera to capture receipt data on the go. Workers will be able to generate expense reports from their phones without having to spend time entering data into web forms; the Form Recognizer tools will capture the necessary data and, together with user information and device locations, update records automatically.Getting started with Form Recognizer is relatively simple, and with a generous limit of 500 free pages per month, you should be able to see quickly if it works for you. Once up and running, it should provide a useful bridge between pen and paper and the digital world, using photographs or scans to quickly bring form content into your business processes. With the quality of modern phone cameras and their support for computational photography, it’s possible to make form recognition a simple plug-in that takes a photo and uploads it to your recognizer, saving a local copy for your records.Form Recognizer is a tool that quickly shows the benefit of machine learning, with a model that’s designed to work flexibly in a relatively closed domain. Applying Azure’s Cognitive Services to specific business problems makes a lot of sense. Handling a paper-to-digital transition is one of those problems that has long been a blocker to improving business processes. Using machine learning to reduce the cost and time needed to deliver digitization is a win for most businesses, especially if it means that we can use the cameras in our pocket rather than expensive scanners and unreliable OCR software.",https://www.infoworld.com/article/3528784/use-azure-cognitive-services-to-automate-forms-processing.html
Why data-driven businesses need a data catalog,Enterprises need better tools to learn and collaborate around data sources. Data catalogs with pioneering machine learning capabilities can help you tap your valuable data,"Relational databases, data lakes, and NoSQL data stores are powerful at inserting, updating, querying, searching, and processing data. But the ironic aspect of working with data management platforms is they usually don’t provide robust tools or user interfaces to share what’s inside them. They are more like data vaults. You know there’s valuable data inside, but you have no easy way to assess it from the outside.The business challenge is dealing with a multitude of data vaults: multiple enterprise databases, smaller data stores, data centers, clouds, applications, BI tools, APIs, spreadsheets, and open data sources.Sure, you can query a relational database’s metadata for a list of tables, stored procedures, indexes, and other database objects to get a directory. But that is a time-consuming approach that requires technical expertise and only produces a basic listing from a single data source.You can use tools that will reverse engineer data models or provide ways to navigate the metadata. But these tools are more often designed for technologists and mainly used for auditing, documenting, or analyzing databases.In other words, these approaches to query the contents of databases and the tools to extract their metadata are insufficient for today’s data-driven business needs for several reasons:Data catalogs have been around for some time and have become more strategic today as organizations scale big data platforms, operate in hybrid clouds, invest in data science and machine learning programs, and sponsor data-driven organizational behaviors.The first concept to understand about data catalogs is that they are tools for the entire organization to learn and collaborate around data sources. They are important to organizations trying to be more data-driven, ones with data scientists experimenting with machine learning, and others Database engineers, software developers, and other technologists take on responsibilities to integrate data catalogs with the primary enterprise data sources. They also use and contribute to the data catalog, especially when databases are created or updated.In that respect, data catalogs that interface with the majority of an enterprise’s data assets are a single source of truth. They help answer what data exists, how to find the best data sources, how to protect data, and who has expertise. The data catalog includes tools to discover data sources, capture metadata about those sources, search them, and provide some metadata management capabilities.Many data catalogs go beyond the notion of a structured directory. Data catalogs often include relationships between data sources, entities, and objects. Most catalogs track different classes of metadata, especially on confidentiality, privacy, and security. They capture and share information on how different people, departments, and applications utilize data sources. Most data catalogs also include tools to define data dictionaries; some bundle in tools to profile data, cleanse data, and perform other data stewardship functions. Specialized data catalogs also enable or interface with master data management and data lineage capabilities.The market is full of data catalog tools and platforms. Some products grew out of other infrastructure and enterprise data management capabilities. Others represent a new generation of capabilities and focus on ease of use, collaboration, and machine learning differentiators. Naturally, choice will depend on scale, user experience, data science strategy, data architecture, and other organization requirements. Here is a sample of data catalog products:Data catalogs that automate data discovery, enable searching the repository, and provide collaboration tools are the basics. More advanced data catalogs include capabilities in machine learning, natural language processing, and low-code implementations.Machine learning capabilities take on several forms depending on the platform. For example, Collibra is using machine learning to help data stewards classify data. Waterline Data has patented fingerprinting technology that automates the discovery, classification, and management of enterprise data. One of their focus areas is identifying and tagging sensitive data; they claim to reduce the time needed for tagging by 80 percent.Different platforms have different strategies and technical capabilities around data processing. Some only function at a data catalog and metadata level, whereas others have extended data prep, integration, cleansing, and other data operational capabilities.InfoWorks DataFoundryWe’re in the early stages of proactive platforms such as data catalogs that provide governance, operational capabilities, and discovery tools for enterprises with growing data assets. As organizations realize more business value from data and analytics, there will be a greater need to scale and manage data practices. Machine learning capabilities will likely be one area where different data catalog platforms compete.",https://www.infoworld.com/article/3512828/why-data-driven-businesses-need-a-data-catalog.html
Making AI’s arcane neural networks accessible,"Data scientists remain in hot demand, but they will give up more of their core functions this year and beyond to automated tools","We’re only a few weeks into the new year, but already we’re seeing signs that automated machine learning modeling, sometimes known as Specifically, it appears that a promising autoML approach known as “Neural architecture search tools optimize the structure, weights, and hyperparameters of a machine learning model’s algorithmic “neurons” in order to make them more accurate, speedy, and efficient in performing data-driven inferences. This technology has only recently begun to emerge from labs devoted to basic research in AI tools and techniques. The Within the burgeoning autoML space, neural architecture search is showing signs of early commercialization.At CES 2020To see how Deeplite’s tool accomplishes this, check out Another key milestone in the maturation of neural architecture search was Amazon’s recent launch of an open source autoML toolkit with this capability built in. Released the same week as CES, Amazon’s new AutoGluon automates data preparation, model development, hyperparameter tuning, and training within the devops flow of an ML model. It can optimize existing PyTorch and MXNet ML models. It can also interface with existing AI devops pipelines via APIs to automatically tweak an existing ML model and thereby improve its performance of inferencing tasks.Amazon currently has AutoGluon running on Linux platforms but has announced plans for MacOS and Windows support. Available from AutoGluon uses RL to speed automated neural architecture searches using computing resources efficiently. Indeed, RL—as implemented both in AutoGluon and in Deeplite’s solution—is proving to be the most fruitful approach for recent advances in this area, using agent-centric actions and rewards to search the space of optimal neural architectures based on estimates of the performance of trained architectures on unseen data. If you truly want to get into the weeds of how AutoGluon works, check out RL is an up-and-coming alternative to evolutionary algorithms, which have been central to neural architecture search since the 1990s in AI R&D environments. Evolutionary algorithms are still widely used in lab environments such as As autoML data science platforms become prevalent in the enterprise world, neural architecture search tools such as these will be a standard component. However, this capability is still scarcely evident in most AI devops environments.By the end of this year, I predict that more than half of As the decade proceeds, neural architecture search will reduce the need for data scientists to understand the neural-net guts of their ML models. This emerging approach will democratize AI by freeing developers to evolve their skillset away from tweaking arcane algorithms and toward developing powerfully predictive intelligent apps.",https://www.infoworld.com/article/3514568/making-ais-arcane-neural-networks-accessible.html
Deep learning vs. machine learning: Understand the differences,"Both machine learning and deep learning discover patterns in data, but they involve dramatically different techniques","Machine learningMachine learning algorithmsUnsupervised learning is further divided into A classification problem is a supervised learning problem that asks for a choice between two or more classes, usually providing probabilities for each class. Leaving out neural networks and deep learning, which require a much higher level of computing resources, the most common algorithms are Naive Bayes, Decision Tree, Logistic Regression, K-Nearest Neighbors, and Support Vector Machine (SVM). You can also use ensemble methods (combinations of models), such as Random Forest, other Bagging methods, and boosting methods such as AdaBoost and XGBoost.A regression problem is a supervised learning problem that asks the model to predict a number. The simplest and fastest algorithm is linear (least squares) regression, but you shouldn’t stop there, because it often gives you a mediocre result. Other common machine learning regression algorithms (short of neural networks) include Naive Bayes, Decision Tree, K-Nearest Neighbors, LVQ (Learning Vector Quantization), LARS Lasso, Elastic Net, Random Forest, AdaBoost, and XGBoost. You’ll notice that there is some overlap between machine learning algorithms for regression and classification.A clustering problem is an unsupervised learning problem that asks the model to find groups of similar data points. The most popular algorithm is K-Means Clustering; others include Mean-Shift Clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), GMM (Gaussian Mixture Models), and HAC (Hierarchical Agglomerative Clustering).Dimensionality reduction is an unsupervised learning problem that asks the model to drop or combine variables that have little or no effect on the result. This is often used in combination with classification or regression. Dimensionality reduction algorithms include removing variables with many missing values, removing variables with low variance, Decision Tree, Random Forest, removing or combining variables with high correlation, Backward Feature Elimination, Forward Feature Selection, Factor Analysis, and PCA (Principal Component Analysis).Training and evaluation turn supervised learning algorithms into models by optimizing their parameter weights to find the set of values that best matches the ground truth of your data. The algorithms often rely on variants of steepest descent for their optimizers, for example stochastic gradient descent, which is essentially steepest descent performed multiple times from randomized starting points.Common refinements on stochastic gradient descent add factors that correct the direction of the gradient based on momentum, or adjust the learning rate based on progress from one pass through the data (called an There is no such thing as clean data in the wild. To be useful for machine learning, data must be aggressively filtered. For example, you’ll want to:There is a lot more you can do, but it will depend on the data collected. This can be tedious, but if you set up a data cleaning step in your To use categorical data for machine classification, you need to encode the text labels into another form. There are two common encodings.One is To use numeric data for machine regression, you usually need to normalize the data. Otherwise, the numbers with larger ranges might tend to dominate the Euclidian distance between A Part of the art of choosing features is to pick a minimum set of Some of the transformations that people use to construct new features or reduce the dimensionality of feature vectors are simple. For example, subtract The usual practice for supervised machine learning is to split the data set into subsets for The errors in the validation data set can be used to identify stopping criteria, or to drive hyperparameter tuning. Most importantly, the errors in the validation data set can help you find out whether the model has overfit the training data.Prediction against the test data set is typically done on the final model. If the test data set was never used for training, it is sometimes called the holdout data set.There are several other schemes for splitting the data. One common technique, In Python, Deep learningThe ideas for “artificial” neural networks go back to the 1940s. The essential concept is that a network of artificial neurons built out of interconnected threshold switches can learn to recognize patterns in the same way that an animal brain and nervous system (including the retina) does.The learning occurs basically by strengthening the connection between two neurons when both are active at the same time during training. In modern neural network software this is most commonly a matter of increasing the weight values for the connections between neurons using a rule called How are the neurons modeled? Each has a propagation function that transforms the outputs of the connected neurons, often with a weighted sum. The output of the propagation function passes to an activation function, which fires when its input exceeds a threshold value.In the 1940s and ’50s artificial neurons used a step activation function and were called The output of the activation function can pass to an output function for additional shaping. Often, however, the output function is the identity function, meaning that the output of the activation function is passed to the downstream connected neurons.Now that we know about the neurons, we need to learn about the common neural network topologies. In a feed-forward network, the neurons are organized into distinct layers: one input layer, In a feed-forward network with shortcut connections, some connections can jump over one or more intermediate layers. In recurrent neural networks, neurons can influence themselves, either directly or indirectly through the next layer.Supervised learning of a neural network is done just like any other machine learning: You present the network with groups of training data, compare the network output with the desired output, generate an error vector, and apply corrections to the network based on the error vector. Batches of training data that are run together before applying corrections are called epochs.For those interested in the details, back propagation uses the gradient of the error (or cost) function with respect to the weights and biases of the model to discover the correct direction to minimize the error. Two things control the application of corrections: the optimization algorithm and the learning rate variable. The learning rate variable usually needs to be small to guarantee convergence and avoid causing dead ReLU neurons.Optimizers for neural networks typically use some form of gradient descent algorithm to drive the back propagation, often with a mechanism to help avoid becoming stuck in local minima, such as optimizing randomly selected mini-batches (Stochastic Gradient Descent) and applying ",https://www.infoworld.com/article/3512245/deep-learning-vs-machine-learning-understand-the-differences.html
2 cloud and AI myths you shouldn’t believe,"If you think the cloud will drive data centers to extinction and that AI projects are doomed to fail, think again","In tech circles, we have two primary faults: We’re overly eager to usher in the future and ironically too quick to disregard it when it doesn’t come as fast as we projected. Take, for example, two persistent myths making the rounds today: First, that cloud spend is sending data center spending off a cliff and, second, that AI is overhyped snake oil that mostly fails enterprise buyers.Let’s take those in order.Gartner kicked off the first myth, with analyst The reason is data gravity. While that gravity worked against the cloud for some time (if data lives in the data center it becomes inefficient to push it to the cloud for processing), it’s now having the reverse effect: More and more data is born in the cloud and will be stored, processed, and analyzed there.And yet... data centers aren’t dying.This is the observation Of course, enterprise ambition and reality may be significantly at odds with each other.Or, as But regardless of the reasons, it’s still the case that as hot as cloud has been, roughly 97 percent of all IT spending still remains on-premises. This isn’t to disparage cloud. It’s just to level set on the reality of where we are in enterprise migration.Which brings us to Myth #2.Going back to Gartner, analyst From such survey data was born countless headlines that all basically screamed, “Most AI projects fail.” Implicit in such headlines is an accusation that the technology behind AI is immature. While AI will undoubtedly continue to advance, the fundamental truth is somewhat different.For one thing, as analyst And here youth might be at fault, at least in part.After all, there’s an absolute glut of ill-prepared but over-hyped junior data scientists entering the industry, ready to R their way to success, as AI, in other words, might be a bit more basic than thought. It also might be failing for reasons that have nothing to do with the technology. And maybe, just maybe, it’s not really failing at all. At least, no more so than other IT projects.According to Thomas DinsmoreIn sum, it might be fun to bury AI prematurely, just as we’ve tried to bury data centers long before they’re dead. In each case we’re displaying an understandable, if naive, desire to get to the future as fast as possible, and then an impatience when the future takes time. In cloud and AI, as in so many other things, the truth is much more nuanced than any headline can portray.",https://www.infoworld.com/article/3515729/2-cloud-and-ai-myths-you-shouldnt-believe.html
Cloud AI is like nuclear power  ,"With incredible potential for both good and harm, AI needs worldwide regulation to ensure it isn't misused","In a recent speech, Google and Alphabet CEO Sundar Pichai called for new regulations in the world of AI, with the obvious focus that AI has been commoditized by cloud computing. This is no surprise, now that we’re debating the ethical questions that surround the use of AI technology: most especially, how easily AI can weaponize computing—for businesses as well as bad actors.Pichai highlighted the dangers posed by technologies such as facial recognition and “deepfakes,” in which an existing image or video of a person is replaced with someone else’s likeness using artificial neural networks. He also stressed that any legislation must balance “potential harms ... with social opportunities.”AI is much more powerful today than it was just a few years ago. AI once resided in the realm of supercomputers that cost budget-busting sums to utilize. Cloud computing made AI an on-demand service, affordable for even small businesses. Moreover, there is a huge boom in R&D spending on AI services. AI providers are racing to the top in terms of innovations and the sheer number of features and functions they can offer. This includes knowledge models that are easy to build and train and can easily integrate with new and existing applications.I would make the analogy that AI is much like nuclear power. Both have potential that needs to be captured. Both need limits to ensure they are not misused. Nuclear power provides cheap, carbon-light electricity, and AI has the potential to give us driverless cars and save hundreds of thousands of lives in the healthcare vertical. Don’t both need regulation?Most technology has the potential to be used for good and bad. AI and nuclear power certainly fall into that category. The risk with AI is that some organizations may leverage it for perfectly sound reasons but end up doing ethically questionable things with it.For example, facial recognition in a retail store can build a database of images and personal information that can be sold to marketing firms. It’s one thing to have security cameras always present, but another when they can find out who you are, your marital status, sexuality, demographics, and other information that can be culled using AI-driven big data analytics.The law of unintended consequences is really what’s at stake here. If regulations are created and adopted but not implemented worldwide, they will have little effect in limiting the misuse of AI. Public clouds are international. If some pattern of AI usage is illegal in one country, it’s simple to move to another region. We already do that with data processing security. AI processing won’t be any different.",https://www.infoworld.com/article/3516111/cloud-ai-is-like-nuclear-power.html
How to keep bias out of your AI models,"AI models are empty, neutral machines. They will acquire a bias when trained with biased data","Bias in artificial intelligence (AI) is hugely controversial these days. From image classifiers that The risk is that we will use AI to create an army of racist, sexist, foul-mouthed bots that will then come back to haunt us. This is an ethical dilemma. If AI is inherently biased, isn’t it dangerous to rely on it? Will we end up shaping our worst future?Let me clarify one thing first: AI is just a machine. We might anthropomorphize it, but it remains a machine. The process is not dissimilar from when we play with stones at the lake with our kids, and suddenly, a dull run-of-the-mill stone becomes a cute pet stone.Even when playing with your kids, we generally do not forget that a pet stone, however cute, is still just a stone. We should do the same with AI: However humanlike its conversation or its look, we should not forget that it still is just a machine.Some time ago, for example, I worked on a bot project: There are many possible speaking or writing styles. In the case of a bot, you might want it to be friendly, but not excessively so — polite and yet sometimes assertive depending on the situation. The blog post “I went for two possible styles: As part of this teacher bot project, a few months ago I implemented a simple deep learning neural network with a hidden layer of long short-term memory (LSTM) units to generate free text.The network would take a sequence of I did not build the deep learning network from scratch, but instead (following the current trend of finding existing examples on the internet) searched the The network would be trained on an appropriate set of free texts. During deployment, a trigger sentence of M=100 initial characters would be provided, and the network would then continue by itself to assemble its own free text.Just imagine a customer or user has unreasonable yet firmly rooted expectations and demands the impossible. How should I answer? How should the bot answer? The first task was to train the network to be assertive — very assertive to the limit of impolite. Where can I find a set of firm and assertive language to train my network?I ended up training my deep learning LSTM-based network on a set of rap song texts. I figured that rap songs might contain all the sufficiently assertive texts needed for the task.What I got was a very foul-mouthed network; so much so that every time I present this case study to an audience, I have to invite all minors to leave the room. You might think that I had created a sexist, racist, disrespectful — i.e., an openly biased — AI system. It seems I did.Below is one of the rap songs the network generated. The first 100 trigger characters were manually inserted; these are in red. The network-generated text is in gray. The trigger sentence is, of course, important to set the proper tone for the rest of the text. For this particular case, I started with the most boring sentence you could find in the English language: a software license description.It is interesting that, among all possible words and phrases, the neural network chose to include “paying a fee,” “expensive,” “banks,” and “honestly” in this song. The tone might not be the same, but the content tries to comply with the trigger sentence.More details about the construction, training, and deployment of this network can be found in the article “The language might not be the most elegant and formal, but it has a pleasant rhythm to it, mainly due to the rhyming. Notice that for the network to generate rhyming text, the length M of the sequence of past input samples must be sufficient. Rhyming works for M=100 but never for M=50 past characters.In an attempt to reeducate my misbehaving network, I created a new training set that included three theater pieces by Shakespeare: two tragedies (“King Lear” and “Othello”) and one comedy (“Much Ado About Nothing”). I then retrained the network on this new training set.After deployment, the network now produces Shakespearean-like text rather than a rap song — a definite improvement in terms of speech cleanliness and politeness. No more profanities! No more foul language!Again, let’s trigger the free text generation with the start of the software license text and see how Shakespeare would proceed according to our network. Below is the Shakespearean text that the network generated: in red, the first 100 trigger characters that were manually inserted; in gray, the network-generated text.Even in this case, the trigger sentence sets the tone for the next words: “thief,” “save and honest,” and the memorable “Sir, where is the patience now” all correspond to the reading of a software license. However, the speaking style is very different this time.More details about the construction, training, and deployment of this network can be found in “Now, keep in mind that the neural network that generated the Shakespearean-like text was the same neural network that generated the rap songs. Exactly the same. It just trained on a different set of data: rap songs on the one hand, Shakespeare’s theater pieces on the other. As a consequence, the free text produced is very different — as is the bias of the texts generated in production.Summarizing, I created a very foul-mouthed, aggressive, and biased AI system and a very elegant, formal, almost poetic AI system too — at least as far as speaking style goes. The beauty of it is that both are based on Indeed, an AI model is just a machine, like a pet stone is ultimately just a stone. It is a machine that adjusts its parameters (learns) on the basis of the data in the training set. Sexist data in the training set produce a sexist AI model. Racist data in the training set produce a racist AI model. Since data are created by humans, they are also often biased. Thus, the resulting AI systems will also be biased. If the goal is to have a clean, honest, unbiased model, then the training data should be cleaned and stripped of all biases before training.Rosaria Silipo is principal data scientist at —New Tech Forum provides a venue to explore and discuss emerging enterprise technology in unprecedented depth and breadth. The selection is subjective, based on our pick of the technologies we believe to be important and of greatest interest to InfoWorld readers. InfoWorld does not accept marketing collateral for publication and reserves the right to edit all contributed content. Send all inquiries to ",https://www.infoworld.com/article/3514578/how-to-keep-bias-out-of-your-ai-models.html
Artificial intelligence predictions for 2020,"Big changes in machine learning applications, tools, techniques, platforms, and standards are on the horizon","Artificial intelligenceWhat changes in core AI uses, tools, techniques, platforms, and standards are in store for the coming year? Here is what we’re likely to see in 2020.AI hardware accelerators have become a principal competitive battlefront in high tech. Even as Nvidia’s market-leading GPU-based offerings appear poised for further growth and adoption in 2020 and beyond. However, over the coming decade, various non-GPU technologies—including CPUs, ASICs, FPGAs, and neural network processing units—will increase their performance, cost, and power efficiency advantages for various edge applications. With each passing year, Nvidia will draw more competition. As the AI market matures and computing platforms vie for the distinction of being fastest, most scalable, and lowest cost in handling these workloads, industry-standard benchmarks will rise in importance. In the past year, the AI modeling frameworks are the core environments within which data scientists build and train statistically driven computational graphs. In 2020, most working data scientists will probably use some blend of As the decade proceeds, the differences between these frameworks will diminish as data scientists and other users value feature parity over strong functional differentiation. By the same token, more AI tool vendors will provide framework-agnostic modeling platforms, which may offer a new lease on life for older frameworks in danger of dying out. Accelerating the spread of open AI modeling platforms is industry adoption of several abstraction layers—such as By the decade’s end, it will become next to irrelevant which front-end modeling tool you use to build and train your machine learning model. No matter where you build your AI, the end-to-end data science pipeline will automatically format, compile, containerize, and otherwise serve it out for optimal execution anywhere from cloud to edge.This past year saw the maturation of machine learning as a service offerings from AWS, Microsoft, Google, IBM, and others. As this trend intensifies, more business users will rely on cloud providers such as these to supply more of their AI requirements without the need to maintain in-house data science teams. By the end of 2020, SaaS providers will become the predominant suppliers of Every digital business transformation initiative hinges on leveraging the best-fit machine learning models. This requires real-world experimentation in which AI-based processes test alternative machine learning models and automatically promote those that achieve the desired result. By the end of 2020, most enterprises will implement real-world experiments in every customer-facing and back-end business process. As business users flock to cloud providers for AI tooling, capabilities such as those recently launched by Neural networksAI-based natural language understanding has become astonishingly accurate. People are rapidly going hands-free on their mobiles and other devices. As conversational user interfaces gain adoption, users will generate more text through voice inputs. By the end of 2020, more user texts, tweets, and other verbal inputs will be rendered though AI-driven AI is becoming a more salient risk factor in enterprise applications. As enterprises confront an upswell in lawsuits over the socioeconomic biases, privacy violations, and other unfortunate impacts of AI-driven applications, chief legal officers will demand a complete audit trail that reveals how the machine learning models used in enterprise apps were built, trained, and governed.By the end of 2020, chief legal officers in most enterprises will require that their data science teams automatically log every step in the machine learning pipeline while also generating a plain-language explanation of how each model drives automated inferencing. As the decade proceeds, a lack of built-in transparency will become a predominant factor in denying AI project funding.Finally, we can safely assume that calls for regulation of AI-based capabilities in all products—especially those that use personally identifiable information—will grow in the coming years. Apart from the growing emphasis on AI devops transparency, it’s too early to say what impact these future mandates will have on the evolution of the underlying platforms, tools, and technologies.But it appears likely that these regulatory initiatives will only intensify in coming years, regardless of who wins the US presidential election this coming November.",https://www.infoworld.com/article/3509465/artificial-intelligence-predictions-for-2020.html
Wanted: More types of machine learning,"Now that we’re big into machine learning in the cloud, perhaps we should start thinking about how to do it better ","ML (machine learning) is handy stuff. Now that public cloud computing has made it cheap, I’m seeing all types of cloud-based applications applying this technology effectively.Basically, there are three types of machine learning.Of course, many Ph.D. theses and scholarly articles identify other types of artificial intelligence or ML as well. What I’ve listed here are the types supported by most cloud-based ML tools.The issue for me is that the ML groups I’ve mentioned are perhaps limiting. Consider a dynamic combining of all types, with adjusting the approach, type, or algorithm during the processing of the training data, either mass loads or transactions.At issue is use cases that don’t really fit these three categories. For example, we have some labeled data and unlabeled data, and we’re looking for the ML engine to identify both the data itself and patterns in the data. Most of us don’t have perfect training data, and it would be nice if the ML engine itself could sort things out for us.With a few exceptions, we have to pick supervised or unsupervised learning and only solve a portion of the problem, and we may not have the training data needed to make it useful. Moreover, we lack the ability to provide reinforcement learning as the data is used within transactional applications, such as identifying a fraudulent transaction ongoing.There are ways to create an “all of the above” approach, but it entails some pretty heavy-duty work for both the training data and the algorithms. This typically involves the typing of data, and applying the proper algorithm (type of ML) to data, both singular and groups. All this customization means that you’ll have to maintain the ML application, the data, and the means of ML processing. You don’t want to get into that business as enterprise IT.The call here is simple: It’s time to think about what’s next for ML as a larger player in the world of AI. This means finding new ways to do more dynamically, with an eye towards flexibility.",https://www.infoworld.com/article/3511938/wanted-more-types-of-machine-learning.html
Launchable applies machine learning to software testing,"Founded by Jenkins creator Kohsuke Kawaguchi, startup promises to make software testing smarter and faster","Startup Still in stealth mode, Launchable is positioned to offer “smarter” testing and “faster” devops. The goal of the company’s technology is to eliminate slow feedback from tests, allowing users to run only the meaningful subset of tests in an order that minimizes feedback delay.Currently, most software projects run tests all the time, in no particular order, the The Launchable machine learning engine learns which tests are relevant by studying past changes and test results. Information from Git repos and test results from CI systems are refined into more meaningful data and then used to train the engine. The resulting prediction can be used in many ways, depending on where Launchable is deployed in the software development cycle. Launchable can be leveraged in intelligent integration tests, pull request validation, or the local development loop.The company is seeking ",https://www.infoworld.com/article/3516292/launchable-applies-machine-learning-to-software-testing.html
Mitigating the risks of the AI black box,"If we don’t understand how machine learning works, how can we trust it? Increasing model transparency creates risks as well as rewards","Enterprises are placing their highest hopes on One of the biggest concerns around AI is that complex ML-based models often operate as “To mitigate these risks, people are starting to demand greater transparency into how machine learning operates in practice and throughout the entire workflow in which models are built, trained, and deployed. All these tools and techniques help data scientists generate “post-hoc explanations” of which particular data inputs drove which particular algorithmic inferences under various circumstances. However, To mitigate the technical risks of algorithmic transparency, enterprise data professionals should explore the In addition to these risks of a technical nature, enterprises that disclose fully how their machine learning models were built and trained may expose themselves to more lawsuits and regulatory scrutiny. Without sacrificing machine learning transparency, mitigating these broader business risks will require a data science devops practice under which post-hoc algorithmic explanations are automatically generated.Just as important, enterprises will need to continually monitor these explanations for anomalies, such as evidence that they, or the models which they purportedly describe, have been hacked. This is a critical concern, because trust in the entire AI edifice will come tumbling down if the enterprises that build and train machine learning models can’t vouch for the transparency of the models’ official documentation.",https://www.infoworld.com/article/3512060/mitigating-the-risks-of-the-ai-black-box.html
Amazon’s AutoGluon automates deep learning for devs,Amazon has launched an open source toolkit to help developers incorporate artificial intelligence in their applications,"Amazon has created an open source toolkit for Officially launched January 9, AutoGluon lets developers harness machine learning models with image, text, or tabular data sets, sans any need to manually experiment. Developers can achieve strong, predictive performance in their applications.Accessible from AutoGluon capabilities allow users to:In explaining the reasoning behind AutoGluon, Amazon said deployment of deep learning models with state-of-the-art inferencing accuracy typically has required extensive expertise. Developers have had to invest a considerable amount of time and effort into training deep learning models. Despite advancements such as the ",https://www.infoworld.com/article/3513516/amazon-s-autogluon-looks-to-make-deep-learning-easier-for-devs.html
8 misleading AI myths — and the realities behind them,With all of the hype around AI and machine learning come many factual inaccuracies. Let’s separate the truth from the fiction,"Computer sentience has been fodder for Hollywood for decades. Usually it’s for the bad, whether it’s HAL in “2001: A Space Odyssey” or the T-800 in “Terminator.” Occasionally it’s been thoughtful, like Project 2501 in “Ghost in the Shell.” Whatever the plot, few themes elicit such fear and loathing as the robot armies and superintelligent computers we get from movies and TV.The rise of Skynet aside, there are some rational questions and answers to be had. So let’s break down the myths and realities around AI and all of its offshoots, like machine learning and deep learning.This is the single greatest fear, and it is legitimate. AI is being used to automate many boring, repetitive functions in areas as diverse as customer service, data center management, and radiology. Does that mean server admins are out of a job? No, it means they are free to work on more challenging tasks. Some industries might be impacted and some workers may be displaced, but that has happened constantly and regularly. The industrial revolution of the late 1800s caused massive displacement. The car put the horse and buggy industry out of business. Early telephone calls could not be done without an operator, and AT&T had armies of them.“What will happen to people whose jobs are replaced by AI? They move on to other jobs. We’ve done that through all of human history. That’s nothing new,” said David McCall, vice president of innovation for data center operator QTS.“Based on all the work we’re doing with large companies, we’re seeing a displacement of lower level knowledge workers,” said Anthony DeLima, head of digital transformation and U.S. operations for Neoris, a digital business transformation accelerator.“AI is automating a number of tasks done by knowledge workers, operates 24/7 at a higher level of accuracy, and also provides insights and forward looking views where the customers or market are going,” DeLima says. “So the prediction level of AI is in some cases exceeding what people can do.”DeLima has a 33% rule: 33% of knowledge workers make the change easily, 33% want to make the change but require significant training to be a higher level knowledge worker, and 33% are unable to adjust or be retrained. For that last 33%, the change is so great they must move on to something else. “We project onto AI what we would do,” said McCall. “I think the smartest engine on earth is the human brain and we’re not going to build a smarter AI than the human brain. AI isn’t sentient, it isn’t conscious, and I don’t think it will get smarter than us.”There is no artificial intelligence without people; the people who create the algorithms and information that make up AI. We build it, teach it, and give it the tools to make certain decisions on our behalf, notes McCall.“Narrowly, AI can be used in some circles to make decisions faster than humans. That doesn’t mean the decisions are always right or thoughtful or always the right outcome,” McCall said. “Is AI socially aware? How do you teach AI to read the room?” Some decisions can only be made by humans.“The initial wave of AIOps did revolve our event management systems to perform noise reduction based on correlating alerts, like grouping of similar alerts,” said Ciaran Byrne, vice president of product strategy for OpsRamp, developer of AIOps software. This was a significant step forward, given that noise has long hindered the usability of event management systems. But even greater benefits are on the horizon. “The next wave has broadened to other areas of IT Operations such as automation and monitoring/observability,” Byrne said. “Use cases would include intelligent routing of tickets or automation based on learned patterns.” QTS predicts that over the next decade, there is no organization, industry, or business segment that is going to completely avoid being touched by AI. It is a risky proposition not to have an AI plan because your competition certainly will — and they will be able to respond to market changes much quicker.Jay Marwaha, CEO at SYNTASA, developer of behavioral analytics software for customer interactions and behavioral data, agrees. “The customers we deal with feel that AI is the next big thing companies must adopt and grow their top line immediately or reduce their bottom line,” he said.How much of an impact AI has depends on how those companies use AI, Marwaha added, and those that use AI to full effect do very well. “Many companies don’t understand the whole picture going into this. They see the buzzwords, hear other companies taking advantage of it,” he said. “The returns are not always that much, but in some cases, the returns are huge.”Today, radiologists are experts in the evaluation of X-rays, MRIs, CAT scans, and other medical imagery. One of the major efforts of AI is teaching image classifiers to recognize abnormalities like tumors. AI has the ability to scan millions of images to learn to interpret scans faster and more thoroughly than any human could ever achieve.However, a doctor or radiologist will still have the final call in determining a diagnosis. It’s just that a diagnosis may come in minutes instead of days or weeks.Early on, AIOps was perceived as being a “black box,” i.e. a mysterious system that generated output without providing insights into what the underlying algorithm did and why. However, over time we are seeing these solutions mature, and more “white box” approaches that are gaining trust and adoption.“While some systems don’t provide transparency, increasingly software vendors and AI systems are providing more visibility into why they did what they did,” said OpsRamp’s Byrne. “The tricky thing is to provide appropriate transparency, to not overwhelm the user, to gain their trust and understanding,” he said.Unstructured data is worse than structured data because it takes up space. To get rid of it you have to use up resources to sift through it all. For that reason, says QTS’s McCall, unstructured data for the sake of unstructured data can be worse than useless.“What the world is working on now is how do I structure and organize data to mine it and how do I build historical algorithms and paradigms,” McCall said. “A little unstructured data is okay but when we open floodgates on data points, you absolutely have to have a data lake with the ability to organize and structure it later.” All AI initiatives begin as test projects. You may get excellent results during the testing phase, but find that your model is far less accurate when you deploy it into production. That’s because AI and machine learning models must be trained on data, and that training data must be representative of the real data, or results will suffer.Note too that the training of your AI model is never complete. As soon as you put your model to use in the real world, its accuracy will begin to degrade. The speed of decline will depend on how fast the real world data changes (and customer preferences can change quickly), but sooner or later your model will have to be retrained with new data that represents the new state of the world.  “It’s a delicate task of defining your training data set. Your training data has to be the same as your production data,” said Marwaha. “That is the key to making your programs successful.” It’s a key you will need to turn to again and again throughout the lifetime of your AI model. ",https://www.infoworld.com/article/3514577/8-misleading-ai-myths-and-the-realities-behind-them.html
Launchable applies machine learning to software testing,"Founded by Jenkins creator Kohsuke Kawaguchi, startup promises to make software testing smarter and faster","Startup Still in stealth mode, Launchable is positioned to offer “smarter” testing and “faster” devops. The goal of the company’s technology is to eliminate slow feedback from tests, allowing users to run only the meaningful subset of tests in an order that minimizes feedback delay.Currently, most software projects run tests all the time, in no particular order, the The Launchable machine learning engine learns which tests are relevant by studying past changes and test results. Information from Git repos and test results from CI systems are refined into more meaningful data and then used to train the engine. The resulting prediction can be used in many ways, depending on where Launchable is deployed in the software development cycle. Launchable can be leveraged in intelligent integration tests, pull request validation, or the local development loop.The company is seeking ",https://www.infoworld.com/article/3516397/launchable-applies-machine-learning-to-software-testing.html
JetBrains taps machine learning for full-line code completion,IntelliJ IDE's 2020 roadmap also features collaborative editing and lightweight text editing,"JetBrains has laid out a 2020 roadmap for The company said the new machine learning-based code completion capabilities would make better use of the context for ranking completion suggestions and generate completion variants that go beyond a single identifier to provide full-line completion. Considered a major area of investment, full-line completion may take a while to appear in the product.JetBrains already had been exploring the use of machine learning for code completion, and some results of that research have made their way into products. IntelliJ now uses machine learning to improve the ranking of completion variants, and language plug-ins tag each produced completion variant with different attributes. IntelliJ also uses machine learning to determine which attributes contribute to item ranking so the most-relevant items are at the top of the list.In addition to machine learning based code completion, JetBrains cited a multitude of improvements to IntellIj for 2020, subject to change. These include:",https://www.infoworld.com/article/3518452/jetbrains-taps-machine-learning-for-full-line-code-completion.html
Deep learning vs. machine learning: Understand the differences,"Both machine learning and deep learning discover patterns in data, but involve dramatically different techniques","Machine learning[ Machine learning algorithmsUnsupervised learning is further divided into A classification problem is a supervised learning problem that asks for a choice between two or more classes, usually providing probabilities for each class. Leaving out neural networks and deep learning, which require a much higher level of computing resources, the most common algorithms are Naive Bayes, Decision Tree, Logistic Regression, K-Nearest Neighbors, and Support Vector Machine (SVM). You can also use ensemble methods (combinations of models), such as Random Forest, other Bagging methods, and boosting methods such as AdaBoost and XGBoost.A regression problem is a supervised learning problem that asks the model to predict a number. The simplest and fastest algorithm is linear (least squares) regression, but you shouldn’t stop there, because it often gives you a mediocre result. Other common machine learning regression algorithms (short of neural networks) include Naive Bayes, Decision Tree, K-Nearest Neighbors, LVQ (Learning Vector Quantization), LARS Lasso, Elastic Net, Random Forest, AdaBoost, and XGBoost. You’ll notice that there is some overlap between machine learning algorithms for regression and classification.A clustering problem is an unsupervised learning problem that asks the model to find groups of similar data points. The most popular algorithm is K-Means Clustering; others include Mean-Shift Clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), GMM (Gaussian Mixture Models), and HAC (Hierarchical Agglomerative Clustering).Dimensionality reduction is an unsupervised learning problem that asks the model to drop or combine variables that have little or no effect on the result. This is often used in combination with classification or regression. Dimensionality reduction algorithms include removing variables with many missing values, removing variables with low variance, Decision Tree, Random Forest, removing or combining variables with high correlation, Backward Feature Elimination, Forward Feature Selection, Factor Analysis, and PCA (Principal Component Analysis).Training and evaluation turn supervised learning algorithms into models by optimizing their parameter weights to find the set of values that best matches the ground truth of your data. The algorithms often rely on variants of steepest descent for their optimizers, for example stochastic gradient descent, which is essentially steepest descent performed multiple times from randomized starting points.Common refinements on stochastic gradient descent add factors that correct the direction of the gradient based on momentum, or adjust the learning rate based on progress from one pass through the data (called an There is no such thing as clean data in the wild. To be useful for machine learning, data must be aggressively filtered. For example, you’ll want to:There is a lot more you can do, but it will depend on the data collected. This can be tedious, but if you set up a data cleaning step in your To use categorical data for machine classification, you need to encode the text labels into another form. There are two common encodings.One is To use numeric data for machine regression, you usually need to normalize the data. Otherwise, the numbers with larger ranges might tend to dominate the Euclidian distance between A Part of the art of choosing features is to pick a minimum set of Some of the transformations that people use to construct new features or reduce the dimensionality of feature vectors are simple. For example, subtract The usual practice for supervised machine learning is to split the data set into subsets for The errors in the validation data set can be used to identify stopping criteria, or to drive hyperparameter tuning. Most importantly, the errors in the validation data set can help you find out whether the model has overfit the training data.Prediction against the test data set is typically done on the final model. If the test data set was never used for training, it is sometimes called the holdout data set.There are several other schemes for splitting the data. One common technique, In Python, Deep learningThe ideas for “artificial” neural networks go back to the 1940s. The essential concept is that a network of artificial neurons built out of interconnected threshold switches can learn to recognize patterns in the same way that an animal brain and nervous system (including the retina) does.The learning occurs basically by strengthening the connection between two neurons when both are active at the same time during training. In modern neural network software this is most commonly a matter of increasing the weight values for the connections between neurons using a rule called How are the neurons modeled? Each has a propagation function that transforms the outputs of the connected neurons, often with a weighted sum. The output of the propagation function passes to an activation function, which fires when its input exceeds a threshold value.In the 1940s and ’50s artificial neurons used a step activation function and were called The output of the activation function can pass to an output function for additional shaping. Often, however, the output function is the identity function, meaning that the output of the activation function is passed to the downstream connected neurons.Now that we know about the neurons, we need to learn about the common neural network topologies. In a feed-forward network, the neurons are organized into distinct layers: one input layer, In a feed-forward network with shortcut connections, some connections can jump over one or more intermediate layers. In recurrent neural networks, neurons can influence themselves, either directly or indirectly through the next layer.Supervised learning of a neural network is done just like any other machine learning: You present the network with groups of training data, compare the network output with the desired output, generate an error vector, and apply corrections to the network based on the error vector. Batches of training data that are run together before applying corrections are called epochs.For those interested in the details, back propagation uses the gradient of the error (or cost) function with respect to the weights and biases of the model to discover the correct direction to minimize the error. Two things control the application of corrections: the optimization algorithm and the learning rate variable. The learning rate variable usually needs to be small to guarantee convergence and avoid causing dead ReLU neurons.",https://www.infoworld.com/article/3512638/deep-learning-vs-machine-learning-understand-the-differences.html
How AI will improve API security,"By using AI models to continuously inspect all API activity, we can fill in the cracks left by policy-based API protections","APIs have become the crown jewels of organizations’ digital transformation initiatives, empowering employees, partners, customers, and other stakeholders to access applications, data, and business functionality across their digital ecosystem. So, it’s no wonder that hackers have increased their waves of attacks against these critical enterprise assets.Unfortunately, it looks like the problem will only worsen. Many enterprises have responded by implementing API management solutions that provide mechanisms, such as authentication, authorization, and throttling. These are must-have capabilities for controlling who accesses APIs across the API ecosystem—and how often. However, in building their internal and external API strategies, organizations also need to address the growth of more sophisticated attacks on APIs by implementing dynamic, artificial intelligence (AI) driven security.This article examines API management and security tools that organizations should incorporate to ensure security, integrity, and availability across their API ecosystems.Rule-based and policy-based security checks, which can be performed in a static or dynamic manner, are mandatory parts of any API management solution. API gateways serve as the main entry point for API access and therefore typically handle policy enforcement by inspecting incoming requests against policies and rules related to security, rate limits, throttling, etc. Let’s look closer at some static and dynamic security checks to see the additional value they bring.Static security checks do not depend on the request volume or any previous request data, since they usually validate message data against a predefined set of rules or policies. Different static security scans are performed in gateways to block SQL injection, cohesive parsing attacks, entity expansion attacks, and schema poisoning, among others.Meanwhile, static policy checks can be applied to payload scanning, header inspection, and access patterns, among others. For example, SQL injection is a common type of attack performed using payloads. If a user sends a JSON (JavaScript Object Notation) payload, the API gateway can validate this particular request against predefined JSON schema. The gateway also can limit the number of elements or other attributes in content as required to protect against harmful data or text patterns within messages.Dynamic security checks, in contrast to static security scans, are always checking against something that varies over time. Usually this involves validating request data with decisions made using existing data. Examples of dynamic checks include access token validation, anomaly detection, and throttling. These dynamic checks depend heavily on the data volume being sent to the gateway. Sometimes these dynamic checks occur outside the API gateway, and then the decisions are communicated to the gateway. Let’s look at a couple examples.Throttling and rate limiting are important for reducing the impact of attacks, because whenever attackers get access to APIs, the first thing they do is read as much data as possible. Throttling API requests — i.e., limiting access to the data — requires that we keep a count of incoming requests within a specific time window. If a request count exceeds the allocated amount at that time, the gateway can block API calls. With rate limiting, we can limit the concurrent access allowed for a given service.Authentication helps API gateways to identify each user who invokes an API uniquely. Available API gateway solutions generally support basic authentication, OAuth 2.0, JWT (JSON Web Token) security, and certificate-based security. Some gateways also provide an authentication layer on top of that for additional fine-grained permission validation, which is usually based on XACML (eXtensible Access Control Markup Language) style policy definition languages. This is important when an API contains multiple resources that need different levels of access control for each resource.Policy-based approaches around authentication, authorization, rate limiting, and throttling are effective tools, but they still leave cracks through which hackers can exploit APIs. Notably, API gateways front multiple web services, and the APIs they manage are frequently loaded with a high number of sessions. Even if we analyzed all those sessions using policies and processes, it would be difficult for a gateway to inspect every request without additional computation power.Additionally, each API has its own access pattern. So, a legitimate access pattern for one API could indicate malicious activity for a different API. For example, when someone buys items through an online shopping application, they will conduct multiple searches before making the purchase. So, a single user sending 10 to 20 requests to a search API within a short period of time can be a legitimate access pattern for a search API. However, if the same user sends multiple requests to the buying API, the access pattern could indicate malicious activity, such as a hacker trying to withdraw as much as possible using a stolen credit card. Therefore, each API access pattern needs to be analyzed separately to determine the correct response.Yet another factor is that significant numbers of attacks happen internally. Here, users with valid credentials and access to systems utilize their ability to attack those systems. Policy-based authentication and authorization capabilities are not designed to prevent these kinds of attacks. Even if we could apply more rules and policies to an API gateway to protect against the attacks described here, the additional overhead on the API gateway would be unacceptable. Enterprises cannot afford to frustrate genuine users by asking them to bear the processing delays of their API gateways. Instead, gateways need to process valid requests without blocking or slowing user API calls.To fill the cracks left by policy-based API protections, modern security teams need artificial intelligence-based API security that can detect and respond to dynamic attacks and the unique vulnerabilities of each API. By applying AI models to continuously inspect and report on all API activity, enterprises could automatically discover anomalous API activity and threats across API infrastructures that traditional methods miss.Even in cases where standard security measures are able to detect anomalies and risks, it can take months to make the discoveries. By contrast, using pre-built models based on user access patterns, an AI-driven security layer would make it possible to detect some attacks in near real time.Importantly, AI engines usually run outside of API gateways and communicate their decisions to them. Because the API gateway does not have to expend resources to process these requests, the addition of AI-security typically does not impact runtime performance.When adding AI-powered security to an API management implementation, there will be a security enforcement point and a decision point. Typically, these units are independent due to the high computational power required, but the latency should not be allowed to affect their efficiency.The API gateway intercepts API requests and applies various policies. Linked to it is the security enforcement point, which describes the attributes of each request (API call) to the decision point, requests a security decision, and then enforces that decision in the gateway. The decision point, powered by AI, continuously learns the behavior of each API access pattern, detects anomalous behaviors, and flags different attributes of the request.There should be an option to add policies to the decision point as needed and invoke these policies—which may vary from API to API—during the learning period. Any policies should be defined by the security team once the potential vulnerabilities of each API they plan to expose are thoroughly understood. However, even without support from external policies, adaptive, AI-powered decision point and enforcement point technology will eventually learn and prevent some of the complex attacks that we cannot detect with policies.Another advantage of having two separate security enforcement point and decision point components is the ability to integrate with existing API management solutions. A simple user interface enhancement and customized extension could integrate the security enforcement point to the API publisher and gateway. From the UI, the API publisher could choose whether to enable AI security for the published API, along with any special policies that needed. The extended security enforcement point would publish the request attributes to the decision point and restrict access to the API according to the decision point’s response.However, publishing events to the decision point and restricting access based on its response will take time and depend heavily on the network. Therefore, it is best implemented asynchronously with the help of a caching mechanism. This will affect the accuracy a bit, but when considering the efficiency of the gateway, adding an AI security layer will minimally contribute to the overall latency.Of course, benefits don’t come without costs. While an AI-driven security layer offers an additional level of API protection, it presents some challenges that security teams will need to address.Given the critical role of APIs in enterprises today, they increasingly are becoming targets for hackers and malicious users. Policy-based mechanisms, such as authentication, authorization, payload scanning, schema validation, throttling, and rate limiting, are baseline requirements for implementing a successful API security strategy. However, only by adding AI models to continuously inspect and report on all API activity will enterprises be protected against the most sophisticated security attacks emerging today.Sanjeewa Malalgoda is software architect and associate director of engineering at —New Tech Forum provides a venue to explore and discuss emerging enterprise technology in unprecedented depth and breadth. The selection is subjective, based on our pick of the technologies we believe to be important and of greatest interest to InfoWorld readers. InfoWorld does not accept marketing collateral for publication and reserves the right to edit all contributed content. Send all inquiries to ",https://www.infoworld.com/article/3516595/how-ai-will-improve-api-security.html
